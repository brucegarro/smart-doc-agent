{"id": "attn-arch", "document_id": "3061ea24-ee4b-4cff-be26-846865e76b4a", "query": "What architecture does the paper introduce?", "gold_passages": ["We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely."], "gold_answers": ["The paper introduces the Transformer, a sequence transduction model built entirely on self-attention."]}
{"id": "encoder-depth", "document_id": "3061ea24-ee4b-4cff-be26-846865e76b4a", "query": "How many layers are stacked in the Transformer encoder?", "gold_passages": ["The encoder is composed of a stack of N=6 identical layers."], "gold_answers": ["The encoder uses N = 6 identical layers."]}
{"id": "multihead-uses", "document_id": "3061ea24-ee4b-4cff-be26-846865e76b4a", "query": "List the three ways multi-head attention is used in the Transformer.", "gold_passages": ["We employ multi-head attention in three different ways: in the encoder-decoder attention layers, in the encoder self-attention layers, and in the decoder self-attention layers."], "gold_answers": ["It is used for encoder-decoder attention, encoder self-attention, and masked decoder self-attention."]}
