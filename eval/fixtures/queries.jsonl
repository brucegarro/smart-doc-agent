{"id": "attn-arch", "document_id": "3061ea24-ee4b-4cff-be26-846865e76b4a", "query": "What architecture does the paper introduce?", "gold_passages": ["We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely."], "gold_answers": ["The paper introduces the Transformer, a sequence transduction model built entirely on self-attention."]}
{"id": "encoder-depth", "document_id": "3061ea24-ee4b-4cff-be26-846865e76b4a", "query": "How many layers are stacked in the Transformer encoder?", "gold_passages": ["The encoder is composed of a stack of N=6 identical layers."], "gold_answers": ["The encoder uses N = 6 identical layers."]}
{"id": "multihead-uses", "document_id": "3061ea24-ee4b-4cff-be26-846865e76b4a", "query": "List the three ways multi-head attention is used in the Transformer.", "gold_passages": ["The Transformer uses multi-head attention in three different ways: In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position."], "gold_answers": ["It is used for encoder-decoder attention, encoder self-attention, and masked decoder self-attention."]}
{"id": "bn-internal-shift", "document_id": "doc_batch_normalization", "query": "What phenomenon does batch normalization seek to reduce?", "gold_passages": ["We refer to the change in the distributions of internal nodes of a deep network, in the course of training, as internal covariate shift. Eliminating it offers a promise of faster training. We propose a new mechanism, which we call Batch Normalization, that takes a step towards reducing internal covariate shift, and in doing so dramatically accelerates the training of deep neural nets."], "gold_answers": ["It reduces internal covariate shift by normalizing each layer's inputs."]}
{"id": "resnet-imagenet-error", "document_id": "doc_deep_residual_learning", "query": "What ImageNet test error does the residual net ensemble achieve?", "gold_passages": ["On the Image Net dataset we evaluate residual nets with a depth of up to 152 layers-8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the Image Net testset. This result won the 1st place on the ILSVRC 2015 classification task."], "gold_answers": ["The ensemble reaches a 3.57% error rate on the ImageNet test set."]}
{"id": "bert-acronym", "document_id": "doc_bert", "query": "What does the acronym BERT stand for?", "gold_passages": ["We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers."], "gold_answers": ["It stands for Bidirectional Encoder Representations from Transformers."]}
{"id": "deeplab-decoder", "document_id": "doc_deeplabv3_plus", "query": "What module does DeepLabv3+ add to improve segmentation boundaries?", "gold_passages": ["Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries."], "gold_answers": ["It adds a decoder module that refines the segmentation along object boundaries."]}
{"id": "cluster-overview", "document_id": "doc_cluster_dilute_regime", "query": "what is this paper about", "gold_passages": ["4. CONCLUSIONS In this present work, the nongeneralizability of often-used empirical relationships relating 3-D and projected properties of cluster–dilute aggregates has been highlighted."], "gold_answers": ["It evaluates how simulated cluster-dilute aggregates' projected 2-D measurements compare with their full 3-D morphology across fractal dimensions."], "gold_chunk_ids": ["0ae322f3-6ed6-435a-a9ec-d2d2e9f56a6b"]}
{"id": "cluster-abstract", "document_id": "doc_cluster_dilute_regime", "query": "what is the abstract of this paper", "gold_passages": ["1. INTRODUCTION The three-dimensional (3-D) fractal dimension, D f , is considered to be the key property for defining the complex morphology of fractal-like aggregates made of coagulated spherical primary particles."], "gold_answers": ["It introduces how fractal dimension governs the morphology of soot-like aggregates and frames the study's comparison of 2-D projections with 3-D properties across growth conditions."], "gold_chunk_ids": ["19e034a8-cd17-4669-a04d-0c3c83a5e9d7"]}
{"id": "cluster-regime-definition", "document_id": "doc_cluster_dilute_regime", "query": "What is the cluster dliute regime?", "gold_passages": ["Drawing upon previous findings, this study compares, as a function of D f , the structural properties of simulated cluster–dilute aggregates—aggregates with average aggregate–aggregate separation much larger than the aggregate size—in both two and three dimensions."], "gold_answers": ["It refers to aggregates that are far enough apart that their average separation greatly exceeds their individual size, so they behave as cluster-dilute systems."], "gold_chunk_ids": ["8e8e6cb8-87ac-40af-8931-1e879d430e16"]}
{"id": "cluster-morphology-formula", "document_id": "doc_cluster_dilute_regime", "query": "What formula represents the morphological parameters of a 3-d aggregate?", "gold_passages": ["The fundamental governing equation connecting D f with other morphological parameters of a 3-D aggregate is N = k 0 ( R 3 d g /d p ) D f ."], "gold_answers": ["It uses N = k0 (R^3d_g / d_p)^{D_f}, linking monomer count, radius of gyration, monomer size, and fractal prefactor."], "gold_chunk_ids": ["19e034a8-cd17-4669-a04d-0c3c83a5e9d7"]}
{"id": "cluster-radius-ratio", "document_id": "doc_cluster_dilute_regime", "query": "What did they calculate the ratio of the 2D radius of gyration divided by the 3D radius of gyration to be in the experimental results?", "gold_passages": ["FIG. 1. The ratio of 2-D and 3-D radius of gyration ( R 2 d g /R 3 d g ) as a function of 3-D fractal dimension ( D f ) with median values and lower and upper quartile designations."], "gold_answers": ["They report R2d_g ≈ R3d_g with a ratio of about 0.93 ± 0.3, consistent with earlier findings near 0.91."], "gold_chunk_ids": ["1613c576-e6ad-4775-8bab-59f5129e1c2c"]}
