{"id": "attn-arch", "document_id": "3061ea24-ee4b-4cff-be26-846865e76b4a", "query": "What architecture does the paper introduce?", "gold_chunk_ids": ["df779185-d35e-4c62-95ac-3acefaec8493"], "gold_answers": ["The paper introduces the Transformer, a sequence transduction model built entirely on self-attention."]}
{"id": "encoder-depth", "document_id": "3061ea24-ee4b-4cff-be26-846865e76b4a", "query": "How many layers are stacked in the Transformer encoder?", "gold_chunk_ids": ["d000368e-fefb-4c1f-b6d0-fc7b292faa47"], "gold_answers": ["The encoder uses N = 6 identical layers."]}
{"id": "multihead-uses", "document_id": "3061ea24-ee4b-4cff-be26-846865e76b4a", "query": "List the three ways multi-head attention is used in the Transformer.", "gold_chunk_ids": ["19e1ba3f-d1f5-4087-82ef-40228efa0e32"], "gold_answers": ["It is used for encoder-decoder attention, encoder self-attention, and masked decoder self-attention."]}
