{"id": "attn-arch", "document_id": "3061ea24-ee4b-4cff-be26-846865e76b4a", "query": "What architecture does the paper introduce?", "gold_passages": ["We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely."], "gold_answers": ["The paper introduces the Transformer, a sequence transduction model built entirely on self-attention."]}
{"id": "encoder-depth", "document_id": "3061ea24-ee4b-4cff-be26-846865e76b4a", "query": "How many layers are stacked in the Transformer encoder?", "gold_passages": ["The encoder is composed of a stack of N=6 identical layers."], "gold_answers": ["The encoder uses N = 6 identical layers."]}
{"id": "multihead-uses", "document_id": "3061ea24-ee4b-4cff-be26-846865e76b4a", "query": "List the three ways multi-head attention is used in the Transformer.", "gold_passages": ["The Transformer uses multi-head attention in three different ways: In \"encoder-decoder attention\" layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models such as [38, 2, 9]. The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position."], "gold_answers": ["It is used for encoder-decoder attention, encoder self-attention, and masked decoder self-attention."]}
{"id": "bn-internal-shift", "document_id": "doc_batch_normalization", "query": "What phenomenon does batch normalization seek to reduce?", "gold_passages": ["We refer to the change in the distributions of internal nodes of a deep network, in the course of training, as internal covariate shift. Eliminating it offers a promise of faster training. We propose a new mechanism, which we call Batch Normalization, that takes a step towards reducing internal covariate shift, and in doing so dramatically accelerates the training of deep neural nets."], "gold_answers": ["It reduces internal covariate shift by normalizing each layer's inputs."]}
{"id": "resnet-imagenet-error", "document_id": "doc_deep_residual_learning", "query": "What ImageNet test error does the residual net ensemble achieve?", "gold_passages": ["On the Image Net dataset we evaluate residual nets with a depth of up to 152 layers-8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the Image Net testset. This result won the 1st place on the ILSVRC 2015 classification task."], "gold_answers": ["The ensemble reaches a 3.57% error rate on the ImageNet test set."]}
{"id": "bert-acronym", "document_id": "doc_bert", "query": "What does the acronym BERT stand for?", "gold_passages": ["We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers."], "gold_answers": ["It stands for Bidirectional Encoder Representations from Transformers."]}
{"id": "deeplab-decoder", "document_id": "doc_deeplabv3_plus", "query": "What module does DeepLabv3+ add to improve segmentation boundaries?", "gold_passages": ["Specifically, our proposed model, DeepLabv3+, extends DeepLabv3 by adding a simple yet effective decoder module to refine the segmentation results especially along object boundaries."], "gold_answers": ["It adds a decoder module that refines the segmentation along object boundaries."]}
