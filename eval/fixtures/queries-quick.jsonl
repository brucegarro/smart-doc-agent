{"id": "attn-arch", "document_id": "3061ea24-ee4b-4cff-be26-846865e76b4a", "query": "What architecture does the paper introduce?", "gold_passages": ["We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely."], "gold_answers": ["The paper introduces the Transformer, a sequence transduction model built entirely on self-attention."]}
{"id": "bn-internal-shift", "document_id": "doc_batch_normalization", "query": "What phenomenon does batch normalization seek to reduce?", "gold_passages": ["We refer to the change in the distributions of internal nodes of a deep network, in the course of training, as internal covariate shift. Eliminating it offers a promise of faster training. We propose a new mechanism, which we call Batch Normalization, that takes a step towards reducing internal covariate shift, and in doing so dramatically accelerates the training of deep neural nets."], "gold_answers": ["It reduces internal covariate shift by normalizing each layer's inputs."]}
{"id": "cluster-regime-definition", "document_id": "doc_cluster_dilute_regime", "query": "What is the cluster dliute regime?", "gold_passages": ["Drawing upon previous findings, this study compares, as a function of D f , the structural properties of simulated cluster–dilute aggregates—aggregates with average aggregate–aggregate separation much larger than the aggregate size—in both two and three dimensions."], "gold_answers": ["It refers to aggregates that are far enough apart that their average separation greatly exceeds their individual size, so they behave as cluster-dilute systems."], "gold_chunk_ids": ["8e8e6cb8-87ac-40af-8931-1e879d430e16"]}
