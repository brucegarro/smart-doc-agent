{
  "env": {
    "answer_context_limit": 3,
    "answer_improvement_margin": 0.05,
    "answer_improvement_rate_threshold": 0.5,
    "answer_judge_model": "qwen2.5:1.5b-instruct-q4_K_M",
    "answer_max_tokens": 256,
    "answer_similarity_threshold": 0.55,
    "answer_temperature": 0.2,
    "code_quality_paths": [
      "src",
      "agent"
    ],
    "complexity_average_threshold": 5.0,
    "complexity_function_threshold": 10.0,
    "embedder_model": "BAAI/bge-small-en-v1.5",
    "git_sha": "unknown",
    "ingest_time_per_page_budget_sec": 5.0,
    "maintainability_threshold": 65.0,
    "retrieval_k": 5,
    "text_llm_model": "qwen2.5:7b-instruct-q4_K_M",
    "vlm_model": "qwen2-vl:7b-instruct-q4_K_M"
  },
  "gates": {
    "all": false,
    "answers": false,
    "boot": true,
    "code_quality": false,
    "extraction": true,
    "ingestion": true,
    "math": true,
    "perf": true,
    "queries": true
  },
  "metrics": {
    "overall_score": 0.5,
    "runtime_seconds": 87.26440811157227
  },
  "run_id": "20251018-080356-unknown",
  "scenarios": {
    "answers": {
      "artifacts": {
        "evaluations": [
          {
            "baseline_answer": "I don't know. The provided context is insufficient to answer the question about the specific architecture introduced in the research paper.",
            "baseline_latency_ms": 5587.290586001473,
            "baseline_similarity": 0.25,
            "context_snippets": [],
            "contextual_answer": null,
            "contextual_judge_reason": null,
            "contextual_judge_score": null,
            "contextual_latency_ms": null,
            "contextual_similarity": null,
            "gold_answer": "The paper introduces the Transformer, a sequence transduction model built entirely on self-attention.",
            "id": "attn-arch",
            "improvement": null,
            "missing_context": true,
            "query": "What architecture does the paper introduce?"
          },
          {
            "baseline_answer": "I don't know. The number of layers in a Transformer encoder can vary depending on the specific architecture and implementation details, which are not provided in the question.",
            "baseline_latency_ms": 2805.4332100000465,
            "baseline_similarity": 0.25116279069767444,
            "context_snippets": [],
            "contextual_answer": null,
            "contextual_judge_reason": null,
            "contextual_judge_score": null,
            "contextual_latency_ms": null,
            "contextual_similarity": null,
            "gold_answer": "The encoder uses N = 6 identical layers.",
            "id": "encoder-depth",
            "improvement": null,
            "missing_context": true,
            "query": "How many layers are stacked in the Transformer encoder?"
          },
          {
            "baseline_answer": "I don't know. The question about the specific uses of multi-head attention in the Transformer architecture was not addressed in the provided context, and I do not have that information available without additional resources.",
            "baseline_latency_ms": 3689.4288760086056,
            "baseline_similarity": 0.345679012345679,
            "context_snippets": [],
            "contextual_answer": null,
            "contextual_judge_reason": null,
            "contextual_judge_score": null,
            "contextual_latency_ms": null,
            "contextual_similarity": null,
            "gold_answer": "It is used for encoder-decoder attention, encoder self-attention, and masked decoder self-attention.",
            "id": "multihead-uses",
            "improvement": null,
            "missing_context": true,
            "query": "List the three ways multi-head attention is used in the Transformer."
          },
          {
            "baseline_answer": "Batch normalization seeks to reduce internal covariate shift in neural networks.",
            "baseline_latency_ms": 2395.6677519890945,
            "baseline_similarity": 0.5562913907284768,
            "context_snippets": [],
            "contextual_answer": null,
            "contextual_judge_reason": null,
            "contextual_judge_score": null,
            "contextual_latency_ms": null,
            "contextual_similarity": null,
            "gold_answer": "It reduces internal covariate shift by normalizing each layer's inputs.",
            "id": "bn-internal-shift",
            "improvement": null,
            "missing_context": true,
            "query": "What phenomenon does batch normalization seek to reduce?"
          },
          {
            "baseline_answer": "I don't know. The specific ImageNet test error rate for a residual net ensemble is not provided in the given context, and without additional information from the paper or source, I cannot determine this value accurately.",
            "baseline_latency_ms": 6371.762045004289,
            "baseline_similarity": 0.1824561403508772,
            "context_snippets": [],
            "contextual_answer": null,
            "contextual_judge_reason": null,
            "contextual_judge_score": null,
            "contextual_latency_ms": null,
            "contextual_similarity": null,
            "gold_answer": "The ensemble reaches a 3.57% error rate on the ImageNet test set.",
            "id": "resnet-imagenet-error",
            "improvement": null,
            "missing_context": true,
            "query": "What ImageNet test error does the residual net ensemble achieve?"
          },
          {
            "baseline_answer": "I don't know. The acronym BERT is not defined in the given context, and without additional information, I cannot provide an accurate definition.",
            "baseline_latency_ms": 3366.3268349919235,
            "baseline_similarity": 0.32710280373831774,
            "context_snippets": [],
            "contextual_answer": null,
            "contextual_judge_reason": null,
            "contextual_judge_score": null,
            "contextual_latency_ms": null,
            "contextual_similarity": null,
            "gold_answer": "It stands for Bidirectional Encoder Representations from Transformers.",
            "id": "bert-acronym",
            "improvement": null,
            "missing_context": true,
            "query": "What does the acronym BERT stand for?"
          },
          {
            "baseline_answer": "I don't know. The specific module added by DeepLabv3+ for improving segmentation boundaries is not provided in the given context, and without it, I cannot accurately answer the question based on the available information.",
            "baseline_latency_ms": 6933.662919997005,
            "baseline_similarity": 0.2866666666666667,
            "context_snippets": [],
            "contextual_answer": null,
            "contextual_judge_reason": null,
            "contextual_judge_score": null,
            "contextual_latency_ms": null,
            "contextual_similarity": null,
            "gold_answer": "It adds a decoder module that refines the segmentation along object boundaries.",
            "id": "deeplab-decoder",
            "improvement": null,
            "missing_context": true,
            "query": "What module does DeepLabv3+ add to improve segmentation boundaries?"
          },
          {
            "baseline_answer": "I don't know.",
            "baseline_latency_ms": 751.2596260057762,
            "baseline_similarity": 0.07547169811320754,
            "context_snippets": [],
            "contextual_answer": null,
            "contextual_judge_reason": null,
            "contextual_judge_score": null,
            "contextual_latency_ms": null,
            "contextual_similarity": null,
            "gold_answer": "It evaluates how simulated cluster-dilute aggregates' projected 2-D measurements compare with their full 3-D morphology across fractal dimensions.",
            "id": "cluster-overview",
            "improvement": null,
            "missing_context": true,
            "query": "what is this paper about"
          },
          {
            "baseline_answer": "I don't know. The question asks for the content of a specific paper's abstract, but no context or details about the paper have been provided.",
            "baseline_latency_ms": 3441.020419006236,
            "baseline_similarity": 0.3416149068322981,
            "context_snippets": [],
            "contextual_answer": null,
            "contextual_judge_reason": null,
            "contextual_judge_score": null,
            "contextual_latency_ms": null,
            "contextual_similarity": null,
            "gold_answer": "It introduces how fractal dimension governs the morphology of soot-like aggregates and frames the study's comparison of 2-D projections with 3-D properties across growth conditions.",
            "id": "cluster-abstract",
            "improvement": null,
            "missing_context": true,
            "query": "what is the abstract of this paper"
          },
          {
            "baseline_answer": "I don't know. The term \"cluster dliute regime\" does not correspond to any known scientific or technical terminology that I can confidently identify based on general knowledge.",
            "baseline_latency_ms": 3610.757875998388,
            "baseline_similarity": 0.20298507462686566,
            "context_snippets": [],
            "contextual_answer": null,
            "contextual_judge_reason": null,
            "contextual_judge_score": null,
            "contextual_latency_ms": null,
            "contextual_similarity": null,
            "gold_answer": "It refers to aggregates that are far enough apart that their average separation greatly exceeds their individual size, so they behave as cluster-dilute systems.",
            "id": "cluster-regime-definition",
            "improvement": null,
            "missing_context": true,
            "query": "What is the cluster dliute regime?"
          },
          {
            "baseline_answer": "I don't know. The specific formula for representing the morphological parameters of a 3-D aggregate is not provided in the given context, and without additional information, I cannot determine it from prior knowledge alone.",
            "baseline_latency_ms": 3306.3855840009637,
            "baseline_similarity": 0.25958702064896755,
            "context_snippets": [],
            "contextual_answer": null,
            "contextual_judge_reason": null,
            "contextual_judge_score": null,
            "contextual_latency_ms": null,
            "contextual_similarity": null,
            "gold_answer": "It uses N = k0 (R^3d_g / d_p)^{D_f}, linking monomer count, radius of gyration, monomer size, and fractal prefactor.",
            "id": "cluster-morphology-formula",
            "improvement": null,
            "missing_context": true,
            "query": "What formula represents the morphological parameters of a 3-d aggregate?"
          },
          {
            "baseline_answer": "I don't know. The specific value for the ratio of the 2D radius of gyration to the 3D radius of gyration was not provided in the context available to me.",
            "baseline_latency_ms": 3718.691211004625,
            "baseline_similarity": 0.296875,
            "context_snippets": [],
            "contextual_answer": null,
            "contextual_judge_reason": null,
            "contextual_judge_score": null,
            "contextual_latency_ms": null,
            "contextual_similarity": null,
            "gold_answer": "They report R2d_g \u2248 R3d_g with a ratio of about 0.93 \u00b1 0.3, consistent with earlier findings near 0.91.",
            "id": "cluster-radius-ratio",
            "improvement": null,
            "missing_context": true,
            "query": "What did they calculate the ratio of the 2D radius of gyration divided by the 3D radius of gyration to be in the experimental results?"
          },
          {
            "baseline_answer": "I don't know. The comparison between \"Attention Is All You Need\" and BERT regarding encoder depth and pre-training objectives is not within my current knowledge base.",
            "baseline_latency_ms": 2812.623834994156,
            "baseline_similarity": 0.3333333333333333,
            "context_snippets": [],
            "contextual_answer": null,
            "contextual_judge_reason": null,
            "contextual_judge_score": null,
            "contextual_latency_ms": null,
            "contextual_similarity": null,
            "gold_answer": "Transformer stacks six encoder layers while BERT focuses on bidirectional pre-training objectives.",
            "id": "compare-transformer-bert",
            "improvement": null,
            "missing_context": true,
            "query": "How does \"Attention Is All You Need\" compare with the BERT paper in encoder depth and pre-training objective?"
          },
          {
            "baseline_answer": "I don't know. The specific details from the papers about their methodologies for improving deep network training are not provided in the context, and I rely on the information given to form an answer.",
            "baseline_latency_ms": 2931.637084999238,
            "baseline_similarity": 0.19318181818181818,
            "context_snippets": [],
            "contextual_answer": null,
            "contextual_judge_reason": null,
            "contextual_judge_score": null,
            "contextual_latency_ms": null,
            "contextual_similarity": null,
            "gold_answer": "ResNet introduces residual connections to ease very deep optimization, while batch normalization reduces internal covariate shift to stabilize training.",
            "id": "compare-resnet-batchnorm",
            "improvement": null,
            "missing_context": true,
            "query": "Contrast how the ResNet paper and the Batch Normalization paper improve deep network training."
          }
        ]
      },
      "details": [
        "no contextual answers generated",
        "unable to compute improvement rate",
        "judge could not score contextual answers",
        "missing context for 14 queries"
      ],
      "duration_seconds": 51.75026010700094,
      "metrics": {
        "baseline_latency_ms_avg": 3694.424847142987,
        "baseline_similarity_all": [
          0.25,
          0.25116279069767444,
          0.345679012345679,
          0.5562913907284768,
          0.1824561403508772,
          0.32710280373831774,
          0.2866666666666667,
          0.07547169811320754,
          0.3416149068322981,
          0.20298507462686566,
          0.25958702064896755,
          0.296875,
          0.3333333333333333,
          0.19318181818181818
        ],
        "baseline_similarity_avg": 0.27874340401887016,
        "context_latency_ms_avg": null,
        "context_similarity_all": [],
        "context_similarity_avg": null,
        "contextual_judge_avg": null,
        "contextual_judge_pass_rate": null,
        "contextual_judge_scores": [],
        "evaluations": 14,
        "fixtures_evaluated": 14,
        "fixtures_total": 14,
        "generation_failures": 0,
        "improvements_all": [],
        "mean_improvement": null,
        "missing_context": 14,
        "negative_improvement_rate": null,
        "positive_improvement_rate": null
      },
      "status": "failed"
    },
    "boot": {
      "artifacts": {
        "checks": [
          {
            "detail": "postgres ready",
            "latency_seconds": 0.004081000006408431,
            "name": "postgres",
            "status": "passed"
          },
          {
            "detail": "minio ready (200)",
            "latency_seconds": 0.012157792007201351,
            "name": "minio",
            "status": "passed"
          },
          {
            "detail": "ollama ready (200)",
            "latency_seconds": 0.014033583996933885,
            "name": "ollama",
            "status": "passed"
          },
          {
            "detail": "redis ready",
            "latency_seconds": 0.0008830000006128103,
            "name": "redis",
            "status": "passed"
          },
          {
            "detail": "app healthcheck disabled",
            "latency_seconds": 6.417001713998616e-06,
            "name": "app",
            "status": "warn"
          },
          {
            "detail": "worker assumed ready",
            "latency_seconds": 0.0005847080028615892,
            "name": "worker",
            "status": "passed"
          }
        ]
      },
      "details": [
        "postgres:passed",
        "minio:passed",
        "ollama:passed",
        "redis:passed",
        "app:warn",
        "worker:passed"
      ],
      "duration_seconds": 0.032171875005587935,
      "metrics": {
        "services_checked": 6,
        "services_failed": 0,
        "services_passed": 5,
        "services_warn": 1
      },
      "status": "warn"
    },
    "code_quality": {
      "artifacts": {
        "analysis_errors": [],
        "complexity_violations": [
          {
            "complexity": 14,
            "lineno": 211,
            "name": "_log_wait_results",
            "path": "agent/cli.py",
            "rank": "C"
          },
          {
            "complexity": 14,
            "lineno": 417,
            "name": "_build_scorecard",
            "path": "agent/evaluator/harness.py",
            "rank": "C"
          },
          {
            "complexity": 19,
            "lineno": 204,
            "name": "_evaluate_record",
            "path": "agent/evaluator/answer_runner.py",
            "rank": "C"
          },
          {
            "complexity": 13,
            "lineno": 437,
            "name": "_derive_status",
            "path": "agent/evaluator/answer_runner.py",
            "rank": "C"
          },
          {
            "complexity": 18,
            "lineno": 64,
            "name": "wait_for_jobs",
            "path": "agent/ingestion/batch_runner.py",
            "rank": "C"
          },
          {
            "complexity": 13,
            "lineno": 193,
            "name": "process_pdfs_parallel",
            "path": "agent/ingestion/processor.py",
            "rank": "C"
          },
          {
            "complexity": 18,
            "lineno": 42,
            "name": "generate",
            "path": "agent/llm/text_client.py",
            "rank": "C"
          }
        ],
        "ignore_patterns": [],
        "maintainability_violations": [
          {
            "maintainability_index": 56.078890611071834,
            "path": "agent/config.py"
          },
          {
            "maintainability_index": 26.227889299697917,
            "path": "agent/cli.py"
          },
          {
            "maintainability_index": 45.79304608886986,
            "path": "agent/evaluator/db_manager.py"
          },
          {
            "maintainability_index": 41.68446598399901,
            "path": "agent/evaluator/boot_runner.py"
          },
          {
            "maintainability_index": 14.229109913692405,
            "path": "agent/evaluator/harness.py"
          },
          {
            "maintainability_index": 10.079774683693687,
            "path": "agent/evaluator/answer_runner.py"
          },
          {
            "maintainability_index": 15.173953755374702,
            "path": "agent/evaluator/query_runner.py"
          },
          {
            "maintainability_index": 33.978640516322386,
            "path": "agent/evaluator/quality_runner.py"
          },
          {
            "maintainability_index": 36.890830855349684,
            "path": "agent/evaluator/ingestion_runner.py"
          },
          {
            "maintainability_index": 37.80561849276781,
            "path": "agent/ingestion/batch_runner.py"
          },
          {
            "maintainability_index": 0.0,
            "path": "agent/ingestion/pdf_parser.py"
          },
          {
            "maintainability_index": 24.201219296632853,
            "path": "agent/ingestion/processor.py"
          },
          {
            "maintainability_index": 48.88138227038773,
            "path": "agent/llm/text_client.py"
          },
          {
            "maintainability_index": 36.559662860009716,
            "path": "agent/embedding/chunker.py"
          },
          {
            "maintainability_index": 29.613994375969135,
            "path": "agent/embedding/text_chunks.py"
          },
          {
            "maintainability_index": 56.64848912582871,
            "path": "agent/embedding/embedder.py"
          },
          {
            "maintainability_index": 32.37493620187342,
            "path": "agent/retrieval/query_parser.py"
          },
          {
            "maintainability_index": 43.176274325889096,
            "path": "agent/retrieval/document_catalog.py"
          },
          {
            "maintainability_index": 44.89273233638553,
            "path": "agent/retrieval/search.py"
          },
          {
            "maintainability_index": 58.46081134883798,
            "path": "agent/db/__init__.py"
          }
        ],
        "missing_paths": [
          "/app/src"
        ]
      },
      "details": [
        "missing paths: /app/src",
        "average cyclomatic complexity 3.80 exceeds threshold 3.60",
        "average maintainability index 56.15 below threshold 62.00"
      ],
      "duration_seconds": 0.23648766700353008,
      "metrics": {
        "blocks_analyzed": 420,
        "cyclomatic_complexity_avg": 3.8,
        "cyclomatic_complexity_max": 19,
        "cyclomatic_complexity_rank": "C",
        "files_analyzed": 32,
        "maintainability_index_avg": 56.15,
        "maintainability_index_min": 0.0,
        "threshold_average": 3.6,
        "threshold_function": 12.0,
        "threshold_maintainability": 62.0
      },
      "status": "failed"
    },
    "db_setup": {
      "artifacts": {},
      "details": [
        "created:docdb_eval_20251018_080356_unknown"
      ],
      "duration_seconds": 0.08418191700184252,
      "metrics": {
        "database": "docdb_eval_20251018_080356_unknown"
      },
      "status": "passed"
    },
    "extraction": {
      "artifacts": {
        "fixtures": [
          {
            "document_id": "3061ea24-ee4b-4cff-be26-846865e76b4a",
            "fields": {
              "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
              "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
              ],
              "title": "Attention Is All You Need"
            }
          },
          {
            "document_id": "doc_batch_normalization",
            "fields": {
              "abstract": "Training deep neural networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin.",
              "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
              ],
              "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
            }
          },
          {
            "document_id": "doc_deep_residual_learning",
            "fields": {
              "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers, 8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.",
              "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
              ],
              "title": "Deep Residual Learning for Image Recognition"
            }
          }
        ]
      },
      "details": [
        "extraction scoring not implemented"
      ],
      "duration_seconds": 0.0,
      "metrics": {
        "fixtures_total": 6
      },
      "status": "warn"
    },
    "ingestion": {
      "artifacts": {
        "documents": [
          "4c360270-b735-405f-b5d3-470ee1e8493a",
          "528e4f0f-b4b2-4497-bd6e-64bf0c9823ca",
          "1426a800-e1b3-4401-b309-7b7425d2b4e8",
          "0f9a349b-e28b-4b6d-9f70-e0d8992f4f17",
          "9022db26-0297-47d9-bc44-c243b4364f32",
          "82b680b4-e36b-4fe4-b1f4-ef89cf8c0899"
        ]
      },
      "details": [
        "ingested:4c360270-b735-405f-b5d3-470ee1e8493a",
        "ingested:528e4f0f-b4b2-4497-bd6e-64bf0c9823ca",
        "ingested:1426a800-e1b3-4401-b309-7b7425d2b4e8",
        "ingested:0f9a349b-e28b-4b6d-9f70-e0d8992f4f17",
        "ingested:9022db26-0297-47d9-bc44-c243b4364f32",
        "ingested:82b680b4-e36b-4fe4-b1f4-ef89cf8c0899"
      ],
      "duration_seconds": 0.0,
      "metrics": {
        "avg_time_per_doc_sec": 0.0,
        "document_ids": [
          "4c360270-b735-405f-b5d3-470ee1e8493a",
          "528e4f0f-b4b2-4497-bd6e-64bf0c9823ca",
          "1426a800-e1b3-4401-b309-7b7425d2b4e8",
          "0f9a349b-e28b-4b6d-9f70-e0d8992f4f17",
          "9022db26-0297-47d9-bc44-c243b4364f32",
          "82b680b4-e36b-4fe4-b1f4-ef89cf8c0899"
        ],
        "documents_attempted": 6,
        "documents_failed": 0,
        "documents_ingested": 6,
        "durations_sec": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "ingest_time_per_page_sec": 0.0,
        "total_pages": 79,
        "total_time_sec": 0.0
      },
      "status": "passed"
    },
    "math": {
      "artifacts": {
        "fixtures": [
          {
            "document_id": "3061ea24-ee4b-4cff-be26-846865e76b4a",
            "id": "scaled-attention",
            "latex_gold": "\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V",
            "notes": "Scaled dot-product attention from Section 3.2."
          }
        ]
      },
      "details": [
        "math scoring not implemented"
      ],
      "duration_seconds": 0.0,
      "metrics": {
        "fixtures_total": 1
      },
      "status": "warn"
    },
    "perf": {
      "artifacts": {},
      "details": [],
      "duration_seconds": 0.0,
      "metrics": {
        "ingestion_avg_time_sec": 0.0,
        "ingestion_total_time_sec": 0.0,
        "query_p50_ms": 1.0682295032893308,
        "query_p95_ms": 4.536049548914887
      },
      "status": "passed"
    },
    "queries": {
      "artifacts": {
        "queries": [
          {
            "error": "column \"ingested_at\" does not exist\nLINE 9:         ORDER BY ingested_at DESC NULLS LAST\n                         ^",
            "hit_at_k": null,
            "id": "attn-arch",
            "latency_ms": 8.706666005309671,
            "match_strategy": "error",
            "ndcg_at_k": null,
            "query": "What architecture does the paper introduce?",
            "retrieved_fingerprints": [],
            "retrieved_snippets": []
          },
          {
            "error": "column \"ingested_at\" does not exist\nLINE 9:         ORDER BY ingested_at DESC NULLS LAST\n                         ^",
            "hit_at_k": null,
            "id": "encoder-depth",
            "latency_ms": 2.2903329954715446,
            "match_strategy": "error",
            "ndcg_at_k": null,
            "query": "How many layers are stacked in the Transformer encoder?",
            "retrieved_fingerprints": [],
            "retrieved_snippets": []
          },
          {
            "error": "column \"ingested_at\" does not exist\nLINE 9:         ORDER BY ingested_at DESC NULLS LAST\n                         ^",
            "hit_at_k": null,
            "id": "multihead-uses",
            "latency_ms": 1.4572079962817952,
            "match_strategy": "error",
            "ndcg_at_k": null,
            "query": "List the three ways multi-head attention is used in the Transformer.",
            "retrieved_fingerprints": [],
            "retrieved_snippets": []
          },
          {
            "error": "column \"ingested_at\" does not exist\nLINE 9:         ORDER BY ingested_at DESC NULLS LAST\n                         ^",
            "hit_at_k": null,
            "id": "bn-internal-shift",
            "latency_ms": 1.1920000106329098,
            "match_strategy": "error",
            "ndcg_at_k": null,
            "query": "What phenomenon does batch normalization seek to reduce?",
            "retrieved_fingerprints": [],
            "retrieved_snippets": []
          },
          {
            "error": "column \"ingested_at\" does not exist\nLINE 9:         ORDER BY ingested_at DESC NULLS LAST\n                         ^",
            "hit_at_k": null,
            "id": "resnet-imagenet-error",
            "latency_ms": 1.3340419973246753,
            "match_strategy": "error",
            "ndcg_at_k": null,
            "query": "What ImageNet test error does the residual net ensemble achieve?",
            "retrieved_fingerprints": [],
            "retrieved_snippets": []
          },
          {
            "error": "column \"ingested_at\" does not exist\nLINE 9:         ORDER BY ingested_at DESC NULLS LAST\n                         ^",
            "hit_at_k": null,
            "id": "bert-acronym",
            "latency_ms": 1.1996249959338456,
            "match_strategy": "error",
            "ndcg_at_k": null,
            "query": "What does the acronym BERT stand for?",
            "retrieved_fingerprints": [],
            "retrieved_snippets": []
          },
          {
            "error": "column \"ingested_at\" does not exist\nLINE 9:         ORDER BY ingested_at DESC NULLS LAST\n                         ^",
            "hit_at_k": null,
            "id": "deeplab-decoder",
            "latency_ms": 0.985249993391335,
            "match_strategy": "error",
            "ndcg_at_k": null,
            "query": "What module does DeepLabv3+ add to improve segmentation boundaries?",
            "retrieved_fingerprints": [],
            "retrieved_snippets": []
          },
          {
            "error": "column \"ingested_at\" does not exist\nLINE 9:         ORDER BY ingested_at DESC NULLS LAST\n                         ^",
            "hit_at_k": null,
            "id": "cluster-overview",
            "latency_ms": 0.965541010373272,
            "match_strategy": "error",
            "ndcg_at_k": null,
            "query": "what is this paper about",
            "retrieved_fingerprints": [],
            "retrieved_snippets": []
          },
          {
            "error": "column \"ingested_at\" does not exist\nLINE 9:         ORDER BY ingested_at DESC NULLS LAST\n                         ^",
            "hit_at_k": null,
            "id": "cluster-abstract",
            "latency_ms": 0.9292090107919648,
            "match_strategy": "error",
            "ndcg_at_k": null,
            "query": "what is the abstract of this paper",
            "retrieved_fingerprints": [],
            "retrieved_snippets": []
          },
          {
            "error": "column \"ingested_at\" does not exist\nLINE 9:         ORDER BY ingested_at DESC NULLS LAST\n                         ^",
            "hit_at_k": null,
            "id": "cluster-regime-definition",
            "latency_ms": 1.028291997499764,
            "match_strategy": "error",
            "ndcg_at_k": null,
            "query": "What is the cluster dliute regime?",
            "retrieved_fingerprints": [],
            "retrieved_snippets": []
          },
          {
            "error": "column \"ingested_at\" does not exist\nLINE 9:         ORDER BY ingested_at DESC NULLS LAST\n                         ^",
            "hit_at_k": null,
            "id": "cluster-morphology-formula",
            "latency_ms": 0.9767080046003684,
            "match_strategy": "error",
            "ndcg_at_k": null,
            "query": "What formula represents the morphological parameters of a 3-d aggregate?",
            "retrieved_fingerprints": [],
            "retrieved_snippets": []
          },
          {
            "error": "column \"ingested_at\" does not exist\nLINE 9:         ORDER BY ingested_at DESC NULLS LAST\n                         ^",
            "hit_at_k": null,
            "id": "cluster-radius-ratio",
            "latency_ms": 1.1081670090788975,
            "match_strategy": "error",
            "ndcg_at_k": null,
            "query": "What did they calculate the ratio of the 2D radius of gyration divided by the 3D radius of gyration to be in the experimental results?",
            "retrieved_fingerprints": [],
            "retrieved_snippets": []
          },
          {
            "error": "column \"ingested_at\" does not exist\nLINE 9:         ORDER BY ingested_at DESC NULLS LAST\n                         ^",
            "hit_at_k": null,
            "id": "compare-transformer-bert",
            "latency_ms": 0.913249998120591,
            "match_strategy": "error",
            "ndcg_at_k": null,
            "query": "How does \"Attention Is All You Need\" compare with the BERT paper in encoder depth and pre-training objective?",
            "retrieved_fingerprints": [],
            "retrieved_snippets": []
          },
          {
            "error": "column \"ingested_at\" does not exist\nLINE 9:         ORDER BY ingested_at DESC NULLS LAST\n                         ^",
            "hit_at_k": null,
            "id": "compare-resnet-batchnorm",
            "latency_ms": 0.9166249947156757,
            "match_strategy": "error",
            "ndcg_at_k": null,
            "query": "Contrast how the ResNet paper and the Batch Normalization paper improve deep network training.",
            "retrieved_fingerprints": [],
            "retrieved_snippets": []
          }
        ]
      },
      "details": [
        "no gold references provided; metrics skipped"
      ],
      "duration_seconds": 0.02400291601952631,
      "metrics": {
        "hit_at_k_avg": null,
        "latency_ms_all": [
          8.706666005309671,
          2.2903329954715446,
          1.4572079962817952,
          1.1920000106329098,
          1.3340419973246753,
          1.1996249959338456,
          0.985249993391335,
          0.965541010373272,
          0.9292090107919648,
          1.028291997499764,
          0.9767080046003684,
          1.1081670090788975,
          0.913249998120591,
          0.9166249947156757
        ],
        "latency_p50_ms": 1.0682295032893308,
        "latency_p95_ms": 4.536049548914887,
        "ndcg_at_k_avg": null,
        "queries_run": 14,
        "top_similarity_all": [],
        "top_similarity_avg": null
      },
      "status": "warn"
    }
  },
  "timestamp": "2025-10-18T08:05:23.272883+00:00"
}