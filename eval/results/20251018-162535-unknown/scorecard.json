{
  "env": {
    "answer_context_limit": 3,
    "answer_improvement_margin": 0.05,
    "answer_improvement_rate_threshold": 0.5,
    "answer_judge_model": "qwen2.5:1.5b-instruct-q4_K_M",
    "answer_max_tokens": 256,
    "answer_similarity_threshold": 0.55,
    "answer_temperature": 0.2,
    "code_quality_paths": [
      "src",
      "agent"
    ],
    "complexity_average_threshold": 5.0,
    "complexity_function_threshold": 10.0,
    "embedder_model": "BAAI/bge-small-en-v1.5",
    "git_sha": "unknown",
    "ingest_time_per_page_budget_sec": 5.0,
    "maintainability_threshold": 65.0,
    "retrieval_k": 5,
    "text_llm_model": "qwen2.5:7b-instruct-q4_K_M",
    "vlm_model": "qwen2-vl:7b-instruct-q4_K_M"
  },
  "gates": {
    "all": false,
    "answers": false,
    "boot": true,
    "code_quality": false,
    "extraction": true,
    "ingestion": true,
    "math": true,
    "perf": true,
    "queries": false
  },
  "metrics": {
    "overall_score": 0.4,
    "runtime_seconds": 242.5000479221344
  },
  "run_id": "20251018-162535-unknown",
  "scenarios": {
    "answers": {
      "artifacts": {
        "evaluations": [
          {
            "baseline_answer": "I don't know. The provided context is insufficient to answer the question about the specific architecture introduced in the paper.",
            "baseline_latency_ms": 8388.15937799518,
            "baseline_similarity": 0.23376623376623376,
            "context_snippets": [
              "Appendix Variant of the Inception Model Used Figure 5 documents the changes that were performed compared to the architecture with respect to the GoogleNet archictecture. For the interpretation of t...",
              "14 \u00d7 14 \u00d7 1024 3 0 128 192 192 256 max + pass through inception (5a) 7 \u00d7 7 \u00d7 1024 3 352 192 320 160 224 avg + 128 inception (5b) 7 \u00d7 7 \u00d7 1024 3 352 192 320 192 224 max + 128 avg pool 7 \u00d7 7 / 1 1 \u00d7...",
              "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Ne..."
            ],
            "contextual_answer": "The paper introduces an Appendix Variant of the Inception Model, which includes modifications compared to the original GoogleNet architecture as detailed in Figure 5.",
            "contextual_judge_reason": "The candidate answer does not match the gold answer. The paper introduces the Transformer model, not an Appendix Variant of Inception Model.",
            "contextual_judge_score": 0.0,
            "contextual_latency_ms": 6807.576295002946,
            "contextual_similarity": 0.37453183520599254,
            "gold_answer": "The paper introduces the Transformer, a sequence transduction model built entirely on self-attention.",
            "id": "attn-arch",
            "improvement": 0.14076560143975878,
            "missing_context": false,
            "query": "What architecture does the paper introduce?"
          },
          {
            "baseline_answer": "I don't know. The number of layers in a Transformer encoder can vary depending on the specific architecture and research paper being referred to.",
            "baseline_latency_ms": 2562.540459999582,
            "baseline_similarity": 0.21621621621621623,
            "context_snippets": [
              "Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decode...",
              "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise...",
              "Most competitive neural sequence transduction models have an encoder-decoder structure [ 5 , 2 , 35 ]. Here, the encoder maps an input sequence of symbol representations ( x 1 , ..., x n ) to a seq..."
            ],
            "contextual_answer": "The encoder is composed of a stack of N = 6 identical layers.",
            "contextual_judge_reason": "The candidate answer accurately reflects the information provided in the gold answer, stating that the encoder uses N = 6 identical layers.",
            "contextual_judge_score": 1.0,
            "contextual_latency_ms": 3913.59900200041,
            "contextual_similarity": 0.7722772277227723,
            "gold_answer": "The encoder uses N = 6 identical layers.",
            "id": "encoder-depth",
            "improvement": 0.5560610115065561,
            "missing_context": false,
            "query": "How many layers are stacked in the Transformer encoder?"
          },
          {
            "baseline_answer": "I don't know. The specific details about how multi-head attention is used in the Transformer model were not provided in the context, and I do not have that information readily available without a reference to the actual paper or additional context.",
            "baseline_latency_ms": 4005.6174600031227,
            "baseline_similarity": 0.3045977011494253,
            "context_snippets": [
              "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. MultiHe...",
              "Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head...",
              "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. of the values, where the weight assigned to each value is compu..."
            ],
            "contextual_answer": "Multi-head attention in the Transformer is used across different representation subspaces at different positions, through several attention layers running in parallel, and as a third sub-layer in addition to two other sub-layers in each decoder layer.",
            "contextual_judge_reason": "The candidate correctly identifies multi-head attention as being used across different representation subspaces at different positions (encoder-decoder attention), encoder self-attention, and masked decoder self-attention, which are indeed the three ways it is used in the Transformer according to the gold answer.",
            "contextual_judge_score": 0.9,
            "contextual_latency_ms": 5956.399918999523,
            "contextual_similarity": 0.3076923076923077,
            "gold_answer": "It is used for encoder-decoder attention, encoder self-attention, and masked decoder self-attention.",
            "id": "multihead-uses",
            "improvement": 0.0030946065428824054,
            "missing_context": false,
            "query": "List the three ways multi-head attention is used in the Transformer."
          },
          {
            "baseline_answer": "Batch normalization seeks to reduce internal covariate shift in neural networks by normalizing the input layer's activations.",
            "baseline_latency_ms": 2375.7180009997683,
            "baseline_similarity": 0.6326530612244898,
            "context_snippets": [
              "In traditional deep networks, too-high learning rate may result in the gradients that explode or vanish, as well as getting stuck in poor local minima. Batch Normaliza- tion helps address these iss...",
              "Interestingly, our method bears similarity to the stan- dardization layer of (G\u00a8ulc\u00b8ehre & Bengio, 2013), though the two methods stem from very different goals, and per- form different tasks. The g...",
              "Simply adding Batch Normalization to a network does not take full advantage of our method. To do so, we further changed the network and its training parameters, as fol- lows: Increase learning rate..."
            ],
            "contextual_answer": "Batch normalization seeks to reduce issues such as gradients exploding or vanishing and getting stuck in poor local minima, which can occur with traditional deep networks.",
            "contextual_judge_reason": "The candidate answer does not directly address what batch normalization seeks to reduce compared to traditional deep networks.",
            "contextual_judge_score": 0.0,
            "contextual_latency_ms": 5646.327752998332,
            "contextual_similarity": 0.21487603305785125,
            "gold_answer": "It reduces internal covariate shift by normalizing each layer's inputs.",
            "id": "bn-internal-shift",
            "improvement": -0.4177770281666386,
            "missing_context": false,
            "query": "What phenomenon does batch normalization seek to reduce?"
          },
          {
            "baseline_answer": "I don't know. The specific ImageNet test error rate for a residual net ensemble is not provided in the given context, and without additional information from the paper or source, I cannot determine this value.",
            "baseline_latency_ms": 4094.9372519971803,
            "baseline_similarity": 0.1897810218978102,
            "context_snippets": [
              "VGG [41] (ILSVRC\u201914) 7.32 GoogLeNet [44] (ILSVRC\u201914) 6.66 VGG [41] (v5) 6.8 PReLU-net [13] 4.94 BN-inception [16] 4.82 ResNet (ILSVRC\u201915) 3.57 Table 5. Error rates (%) of ensembles . The top-5 erro...",
              "Table 2. Top-1 error (%, 10-crop testing) on ImageNet validation. Here the ResNets have no extra parameter compared to their plain counterparts. Fig. 4 shows the training procedures. 34-layer plain...",
              "We evaluate our method on the ImageNet 2012 classi\ufb01- cation dataset [36] that consists of 1000 classes. The models are trained on the 1.28 million training images, and evalu- ated on the 50k valida..."
            ],
            "contextual_answer": "I don't know. The provided context does not specify the exact top-5 error rate for the ResNet ensemble on ImageNet; it only mentions that ResNets have comparable parameters to their plain counterparts and shows training procedures, but no specific test error rates are given.",
            "contextual_judge_reason": "The candidate does not provide a specific error rate for ResNet ensemble on ImageNet test set as mentioned in the gold answer.",
            "contextual_judge_score": 0.0,
            "contextual_latency_ms": 8747.020421011257,
            "contextual_similarity": 0.25882352941176473,
            "gold_answer": "The ensemble reaches a 3.57% error rate on the ImageNet test set.",
            "id": "resnet-imagenet-error",
            "improvement": 0.06904250751395452,
            "missing_context": false,
            "query": "What ImageNet test error does the residual net ensemble achieve?"
          },
          {
            "baseline_answer": "I don't know. The acronym BERT is not defined in the given context, and without additional information, I cannot provide an accurate definition.",
            "baseline_latency_ms": 3183.2843349984614,
            "baseline_similarity": 0.32710280373831774,
            "context_snippets": [
              "We organize the appendix into three sections: \u2022 Additional implementation details for BERT are presented in Appendix A ; \u2022 Additional details for our experiments are presented in Appendix B ; and \u2022...",
              "Ours BERT LARGE (Single) 78.7 81.9 80.0 83.1 Table 3: SQuAD 2.0 results. We exclude entries that use BERT as one of their components. tuning data, we only lose 0.1-0.4 F1, still outper- forming all...",
              "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language { jacobdevlin,mingweichang,kentonl,kris..."
            ],
            "contextual_answer": "BERT stands for \"Pre-training of Deep Bidirectional Transformers for Language Understanding.\"",
            "contextual_judge_reason": "The candidate answer correctly identifies that BERT stands for Bidirectional Encoder Representations from Transformers, which matches the gold answer.",
            "contextual_judge_score": 0.9,
            "contextual_latency_ms": 4949.50946100289,
            "contextual_similarity": 0.49079754601226994,
            "gold_answer": "It stands for Bidirectional Encoder Representations from Transformers.",
            "id": "bert-acronym",
            "improvement": 0.1636947422739522,
            "missing_context": false,
            "query": "What does the acronym BERT stand for?"
          },
          {
            "baseline_answer": "I don't know. The specific module added by DeepLabv3+ for improving segmentation boundaries is not provided in the given context, and my prior knowledge does not specify this detail without additional information.",
            "baseline_latency_ms": 7551.593295997009,
            "baseline_similarity": 0.2945205479452055,
            "context_snippets": [
              "We de\ufb01ne \u201cDeepLabv3 feature map\u201d as the last feature map computed by DeepLabv3 ( i.e ., the features containing ASPP features and image-level fea- tures), and [ k \u00d7 k, f ] as a convolution operatio...",
              "We de\ufb01ne \u201cDeepLabv3 feature map\u201d as the last feature map computed by DeepLabv3 ( i.e ., the features containing ASPP features and image-level fea- tures), and [ k \u00d7 k, f ] as a convolution operatio...",
              "4 L.-C Chen, Y. Zhu, G. Papandreou, F. Schro\ufb00, and H. Adam 1x1 Conv 3x3 Conv rate 6 3x3 Conv rate 12 3x3 Conv rate 18 Image Pooling 1x1 Conv 1x1 Conv Low-Level Features Upsample by 4 Concat 3x3 Con..."
            ],
            "contextual_answer": "I don't know. The provided context does not specify any additional modules added by DeepLabv3+ for improving segmentation boundaries.",
            "contextual_judge_reason": "The candidate's response does not match the provided gold answer which states that DeepLabv3+ adds a decoder module to refine segmentation along object boundaries.",
            "contextual_judge_score": 0.0,
            "contextual_latency_ms": 6372.403377987212,
            "contextual_similarity": 0.39622641509433965,
            "gold_answer": "It adds a decoder module that refines the segmentation along object boundaries.",
            "id": "deeplab-decoder",
            "improvement": 0.10170586714913415,
            "missing_context": false,
            "query": "What module does DeepLabv3+ add to improve segmentation boundaries?"
          },
          {
            "baseline_answer": "I don't know.",
            "baseline_latency_ms": 983.8292510103201,
            "baseline_similarity": 0.07547169811320754,
            "context_snippets": [
              "Aerosol Science and Technology ISSN: 0278-6826 (Print) 1521-7388 (Online) Journal homepage: www.tandfonline.com/journals/uast20",
              "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Ne...",
              "on a 2-D plane were determined. Projections corresponding to the stable orientations for a 3-D aggregate were obtained as 2-D pixilated images and were analyzed for their projected area A agg , pro..."
            ],
            "contextual_answer": "I don't know.\nThe context provided does not contain enough information to determine the specific topic of the research paper. The third snippet mentions analyzing projections of stable orientations for 3-D aggregates, but without more details, I cannot provide a precise answer.",
            "contextual_judge_reason": "The candidate answer does not match any of the provided gold answers. The context mentions analyzing projections of stable orientations for 3-D aggregates but does not provide specific details about how these projections compare with full 3-D morphology across fractal dimensions.",
            "contextual_judge_score": 0.0,
            "contextual_latency_ms": 6688.078752995352,
            "contextual_similarity": 0.2169811320754717,
            "gold_answer": "It evaluates how simulated cluster-dilute aggregates' projected 2-D measurements compare with their full 3-D morphology across fractal dimensions.",
            "id": "cluster-overview",
            "improvement": 0.14150943396226418,
            "missing_context": false,
            "query": "what is this paper about"
          },
          {
            "baseline_answer": "I don't know. The question asks for the content of a specific paper's abstract, but no context or details about the paper were provided.",
            "baseline_latency_ms": 2963.6643339908915,
            "baseline_similarity": 0.3470031545741325,
            "context_snippets": [
              "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Ne...",
              "on a 2-D plane were determined. Projections corresponding to the stable orientations for a 3-D aggregate were obtained as 2-D pixilated images and were analyzed for their projected area A agg , pro...",
              "on a 2-D plane were determined. Projections corresponding to the stable orientations for a 3-D aggregate were obtained as 2-D pixilated images and were analyzed for their projected area A agg , pro..."
            ],
            "contextual_answer": "I don't know. The provided context snippets do not include an abstract but discuss the analysis of projections of 3-D aggregates onto a 2-D plane to determine their projected areas.",
            "contextual_judge_reason": "The candidate's response does not match the provided gold answer which describes an abstract that introduces fractal dimension and compares 2-D projections with 3-D properties across growth conditions.",
            "contextual_judge_score": 0.0,
            "contextual_latency_ms": 8424.532462988282,
            "contextual_similarity": 0.3259668508287293,
            "gold_answer": "It introduces how fractal dimension governs the morphology of soot-like aggregates and frames the study's comparison of 2-D projections with 3-D properties across growth conditions.",
            "id": "cluster-abstract",
            "improvement": -0.021036303745403218,
            "missing_context": false,
            "query": "what is the abstract of this paper"
          },
          {
            "baseline_answer": "I don't know. The term \"cluster dliute regime\" does not correspond to a recognized scientific or technical concept without additional context.",
            "baseline_latency_ms": 3500.522918999195,
            "baseline_similarity": 0.2119205298013245,
            "context_snippets": [
              "2. SIMULATION AND ANALYSIS OF AGGREGATES The simulation technique used for this study makes use of a recently developed simulation package, FracMAP (Chakrabarty et al. 2008, 2009a), which generates...",
              "2. SIMULATION AND ANALYSIS OF AGGREGATES The simulation technique used for this study makes use of a recently developed simulation package, FracMAP (Chakrabarty et al. 2008, 2009a), which generates...",
              "Simulation of Aggregates with Point-Contacting Monomers in the Cluster\u2013Dilute Regime. Part 2: Comparison of Two- and Three-Dimensional Structural Properties as a Function of Fractal Dimension Rajan..."
            ],
            "contextual_answer": "The cluster-dilute regime refers to a specific condition where aggregates are modeled with point-contacting monomers, focusing on comparing two- and three-dimensional structural properties as a function of fractal dimension. I don't know if this is directly defined in the context provided, but it relates to the simu...",
            "contextual_judge_reason": "{\"score\": 0.5, \"explanation\": \"The candidate's description touches on aspects related to the cluster-dilute regime but does not directly match the provided gold answer verbatim. The gold answer specifies that this refers to aggregates where their average separation greatly exceeds their individual size and behaves as a cluster-dilute system, while the candidate mentions focusing on structural properties of aggregates and using point-contacting monomers in simulations.\"",
            "contextual_judge_score": null,
            "contextual_latency_ms": 11450.967170996591,
            "contextual_similarity": 0.0994263862332696,
            "error": "judge:unable to parse score",
            "gold_answer": "It refers to aggregates that are far enough apart that their average separation greatly exceeds their individual size, so they behave as cluster-dilute systems.",
            "id": "cluster-regime-definition",
            "improvement": -0.1124941435680549,
            "missing_context": false,
            "query": "What is the cluster dliute regime?"
          },
          {
            "baseline_answer": "I don't know. The specific formula for representing the morphological parameters of a 3-D aggregate is not provided in the given context, and without additional information, I cannot determine it from general knowledge alone.",
            "baseline_latency_ms": 5233.921001999988,
            "baseline_similarity": 0.25219941348973607,
            "context_snippets": [
              "4. CONCLUSIONS In this present work, the nongeneralizability of often-used empirical relationships relating 3-D and projected properties of cluster\u2013dilute aggregates has been highlighted. This was...",
              "where the superscripts and the subscripts correspond to the difference between upper and lower quartile values and the mean, respectively. It is interesting to note that the as- pect ratio approach...",
              "\u2248 1.2\u20131.5) being produced from a premixed \ufb02ame (Chakrabarty et al. 2009b). Given the varied morphology that aggregates can assume, experimental techniques to characterize their morphology is of pri..."
            ],
            "contextual_answer": "I don't know. The provided context does not specify a particular formula for representing the morphological parameters of a 3-D aggregate.",
            "contextual_judge_reason": "The candidate answer does not provide a specific formula as described by the gold answer.",
            "contextual_judge_score": 0.0,
            "contextual_latency_ms": 5651.425835996633,
            "contextual_similarity": 0.29133858267716534,
            "gold_answer": "It uses N = k0 (R^3d_g / d_p)^{D_f}, linking monomer count, radius of gyration, monomer size, and fractal prefactor.",
            "id": "cluster-morphology-formula",
            "improvement": 0.03913916918742927,
            "missing_context": false,
            "query": "What formula represents the morphological parameters of a 3-d aggregate?"
          },
          {
            "baseline_answer": "I don't know. The specific value for the ratio of the 2D radius of gyration to the 3D radius of gyration was not provided in the context available to me.",
            "baseline_latency_ms": 6819.976252998458,
            "baseline_similarity": 0.296875,
            "context_snippets": [
              "results are still far worse than those of the pre- trained bidirectional models. The BiLSTM hurts performance on the GLUE tasks. We recognize that it would also be possible to train separate LTR an...",
              "The analogous formula is used for the end of the answer span. The score of a candidate span from position i to position j is de\ufb01ned as S \u00b7 T i + E \u00b7 T j , and the maximum scoring span where j \u2265 i i...",
              "We use a batch size of 32 and \ufb01ne-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best \ufb01ne-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set...."
            ],
            "contextual_answer": "I don't know. The provided context does not contain any information about 2D or 3D radii of gyration or their ratios.",
            "contextual_judge_reason": "{\"score\": 0, \"explanation\": \"The candidate answer does not match the information provided in the gold answer, which states that they calculated the ratio of the 2D radius of gyration divided by the 3D radius of gyration to be about 0.93 \u00b1 0.3.\" The candidate's statement is entirely unrelated and incorrect based on the given context.\"",
            "contextual_judge_score": null,
            "contextual_latency_ms": 7622.490629000822,
            "contextual_similarity": 0.33636363636363636,
            "error": "judge:unable to parse score",
            "gold_answer": "They report R2d_g \u2248 R3d_g with a ratio of about 0.93 \u00b1 0.3, consistent with earlier findings near 0.91.",
            "id": "cluster-radius-ratio",
            "improvement": 0.039488636363636365,
            "missing_context": false,
            "query": "What did they calculate the ratio of the 2D radius of gyration divided by the 3D radius of gyration to be in the experimental results?"
          },
          {
            "baseline_answer": "I don't know. The comparison between \"Attention Is All You Need\" and BERT regarding encoder depth and pre-training objectives is not within my current knowledge base.",
            "baseline_latency_ms": 4216.2450430041645,
            "baseline_similarity": 0.3333333333333333,
            "context_snippets": [
              "Dev Set Tasks MNLI-m QNLI MRPC SST-2 SQuAD (Acc) (Acc) (Acc) (Acc) (F1) BERT BASE 84.4 88.4 86.7 92.7 88.5 No NSP 83.9 84.9 86.5 92.6 87.9 LTR & No NSP 82.1 84.3 77.5 92.1 77.8 + BiLSTM 82.1 84.1 7...",
              "ablation studies can be found in Appendix C . 5.1 Effect of Pre-training Tasks We demonstrate the importance of the deep bidi- rectionality of BERT by evaluating two pre- training objectives using...",
              "Fine-tuning is straightforward since the self- attention mechanism in the Transformer al- lows BERT to model many downstream tasks\u2014 whether they involve single text or text pairs\u2014by swapping out th..."
            ],
            "contextual_answer": "I don't know. The provided context does not contain specific information about the comparison between \"Attention Is All You Need\" and BERT regarding encoder depth or pre-training objectives.",
            "contextual_judge_reason": "The candidate does not provide specific information about encoder depth or pre-training objectives compared to BERT.",
            "contextual_judge_score": 0.0,
            "contextual_latency_ms": 10237.141878998955,
            "contextual_similarity": 0.3472222222222222,
            "gold_answer": "Transformer stacks six encoder layers while BERT focuses on bidirectional pre-training objectives.",
            "id": "compare-transformer-bert",
            "improvement": 0.013888888888888895,
            "missing_context": false,
            "query": "How does \"Attention Is All You Need\" compare with the BERT paper in encoder depth and pre-training objective?"
          },
          {
            "baseline_answer": "I don't know. The specific details from the papers about their methodologies to improve deep network training are not provided in the context, and I'm relying on general knowledge which may not be comprehensive enough for an accurate comparison.",
            "baseline_latency_ms": 5449.174295004923,
            "baseline_similarity": 0.19143576826196473,
            "context_snippets": [
              "plies to sub-networks and layers, and removing it from internal activations of the network may aid in training. Our proposed method draws its power from normalizing activations, and from incorporat...",
              "We refer to the change in the distributions of internal nodes of a deep network, in the course of training, as In- ternal Covariate Shift . Eliminating it offers a promise of faster training. We pr...",
              "Abstract Training Deep Neural Networks is complicated by the fact that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change. This slows d..."
            ],
            "contextual_answer": "I don't know. The provided context snippets do not specifically reference the ResNet or Batch Normalization papers, making it impossible to draw a direct comparison between their approaches based on this information alone.",
            "contextual_judge_reason": "The candidate's response does not provide a direct comparison between ResNet and Batch Normalization as requested by the gold answer.",
            "contextual_judge_score": 0.0,
            "contextual_latency_ms": 6713.031003004289,
            "contextual_similarity": 0.24598930481283424,
            "gold_answer": "ResNet introduces residual connections to ease very deep optimization, while batch normalization reduces internal covariate shift to stabilize training.",
            "id": "compare-resnet-batchnorm",
            "improvement": 0.0545535365508695,
            "missing_context": false,
            "query": "Contrast how the ResNet paper and the Batch Normalization paper improve deep network training."
          }
        ]
      },
      "details": [
        "avg similarity with context 0.334",
        "context improvement rate 0.500",
        "judge pass rate 0.250< 0.6",
        "context degraded answers for 14.3% of evaluated queries",
        "2 generation attempts encountered errors"
      ],
      "duration_seconds": 207.89715647000412,
      "metrics": {
        "baseline_latency_ms_avg": 4380.655948499874,
        "baseline_similarity_all": [
          0.23376623376623376,
          0.21621621621621623,
          0.3045977011494253,
          0.6326530612244898,
          0.1897810218978102,
          0.32710280373831774,
          0.2945205479452055,
          0.07547169811320754,
          0.3470031545741325,
          0.2119205298013245,
          0.25219941348973607,
          0.296875,
          0.3333333333333333,
          0.19143576826196473
        ],
        "baseline_similarity_avg": 0.27906260596509985,
        "context_latency_ms_avg": 7084.321711641678,
        "context_similarity_all": [
          0.37453183520599254,
          0.7722772277227723,
          0.3076923076923077,
          0.21487603305785125,
          0.25882352941176473,
          0.49079754601226994,
          0.39622641509433965,
          0.2169811320754717,
          0.3259668508287293,
          0.0994263862332696,
          0.29133858267716534,
          0.33636363636363636,
          0.3472222222222222,
          0.24598930481283424
        ],
        "context_similarity_avg": 0.3341795006721876,
        "contextual_judge_avg": 0.2333333333333333,
        "contextual_judge_pass_rate": 0.25,
        "contextual_judge_scores": [
          0.0,
          1.0,
          0.9,
          0.0,
          0.0,
          0.9,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "evaluations": 14,
        "fixtures_evaluated": 14,
        "fixtures_total": 14,
        "generation_failures": 2,
        "improvements_all": [
          0.14076560143975878,
          0.5560610115065561,
          0.0030946065428824054,
          -0.4177770281666386,
          0.06904250751395452,
          0.1636947422739522,
          0.10170586714913415,
          0.14150943396226418,
          -0.021036303745403218,
          -0.1124941435680549,
          0.03913916918742927,
          0.039488636363636365,
          0.013888888888888895,
          0.0545535365508695
        ],
        "mean_improvement": 0.055116894707087846,
        "missing_context": 0,
        "negative_improvement_rate": 0.14285714285714285,
        "positive_improvement_rate": 0.5
      },
      "status": "failed"
    },
    "boot": {
      "artifacts": {
        "checks": [
          {
            "detail": "postgres ready",
            "latency_seconds": 0.003843375001451932,
            "name": "postgres",
            "status": "passed"
          },
          {
            "detail": "minio ready (200)",
            "latency_seconds": 0.012251958003616892,
            "name": "minio",
            "status": "passed"
          },
          {
            "detail": "ollama ready (200)",
            "latency_seconds": 0.01209666600334458,
            "name": "ollama",
            "status": "passed"
          },
          {
            "detail": "redis ready",
            "latency_seconds": 0.0008220830059144646,
            "name": "redis",
            "status": "passed"
          },
          {
            "detail": "app healthcheck disabled",
            "latency_seconds": 5.249996320344508e-06,
            "name": "app",
            "status": "warn"
          },
          {
            "detail": "worker assumed ready",
            "latency_seconds": 0.0005927499878453091,
            "name": "worker",
            "status": "passed"
          }
        ]
      },
      "details": [
        "postgres:passed",
        "minio:passed",
        "ollama:passed",
        "redis:passed",
        "app:warn",
        "worker:passed"
      ],
      "duration_seconds": 0.029864083000575192,
      "metrics": {
        "services_checked": 6,
        "services_failed": 0,
        "services_passed": 5,
        "services_warn": 1
      },
      "status": "warn"
    },
    "code_quality": {
      "artifacts": {
        "analysis_errors": [],
        "complexity_violations": [
          {
            "complexity": 14,
            "lineno": 211,
            "name": "_log_wait_results",
            "path": "agent/cli.py",
            "rank": "C"
          },
          {
            "complexity": 14,
            "lineno": 437,
            "name": "_build_scorecard",
            "path": "agent/evaluator/harness.py",
            "rank": "C"
          },
          {
            "complexity": 19,
            "lineno": 204,
            "name": "_evaluate_record",
            "path": "agent/evaluator/answer_runner.py",
            "rank": "C"
          },
          {
            "complexity": 13,
            "lineno": 437,
            "name": "_derive_status",
            "path": "agent/evaluator/answer_runner.py",
            "rank": "C"
          },
          {
            "complexity": 18,
            "lineno": 64,
            "name": "wait_for_jobs",
            "path": "agent/ingestion/batch_runner.py",
            "rank": "C"
          },
          {
            "complexity": 13,
            "lineno": 193,
            "name": "process_pdfs_parallel",
            "path": "agent/ingestion/processor.py",
            "rank": "C"
          },
          {
            "complexity": 18,
            "lineno": 42,
            "name": "generate",
            "path": "agent/llm/text_client.py",
            "rank": "C"
          }
        ],
        "ignore_patterns": [],
        "maintainability_violations": [
          {
            "maintainability_index": 56.078890611071834,
            "path": "agent/config.py"
          },
          {
            "maintainability_index": 26.227889299697917,
            "path": "agent/cli.py"
          },
          {
            "maintainability_index": 45.79304608886986,
            "path": "agent/evaluator/db_manager.py"
          },
          {
            "maintainability_index": 41.68446598399901,
            "path": "agent/evaluator/boot_runner.py"
          },
          {
            "maintainability_index": 13.186913770107497,
            "path": "agent/evaluator/harness.py"
          },
          {
            "maintainability_index": 10.079774683693687,
            "path": "agent/evaluator/answer_runner.py"
          },
          {
            "maintainability_index": 15.173953755374702,
            "path": "agent/evaluator/query_runner.py"
          },
          {
            "maintainability_index": 33.978640516322386,
            "path": "agent/evaluator/quality_runner.py"
          },
          {
            "maintainability_index": 36.890830855349684,
            "path": "agent/evaluator/ingestion_runner.py"
          },
          {
            "maintainability_index": 37.80561849276781,
            "path": "agent/ingestion/batch_runner.py"
          },
          {
            "maintainability_index": 0.0,
            "path": "agent/ingestion/pdf_parser.py"
          },
          {
            "maintainability_index": 24.145909038968345,
            "path": "agent/ingestion/processor.py"
          },
          {
            "maintainability_index": 46.14492185676608,
            "path": "agent/llm/text_client.py"
          },
          {
            "maintainability_index": 36.559662860009716,
            "path": "agent/embedding/chunker.py"
          },
          {
            "maintainability_index": 29.613994375969135,
            "path": "agent/embedding/text_chunks.py"
          },
          {
            "maintainability_index": 56.64848912582871,
            "path": "agent/embedding/embedder.py"
          },
          {
            "maintainability_index": 32.37493620187342,
            "path": "agent/retrieval/query_parser.py"
          },
          {
            "maintainability_index": 43.176274325889096,
            "path": "agent/retrieval/document_catalog.py"
          },
          {
            "maintainability_index": 44.89273233638553,
            "path": "agent/retrieval/search.py"
          },
          {
            "maintainability_index": 58.46081134883798,
            "path": "agent/db/__init__.py"
          },
          {
            "maintainability_index": 48.176298475408394,
            "path": "agent/notebook/ingest_ui.py"
          }
        ],
        "missing_paths": [
          "/app/src"
        ]
      },
      "details": [
        "missing paths: /app/src",
        "average cyclomatic complexity 3.80 exceeds threshold 3.60",
        "average maintainability index 57.09 below threshold 62.00"
      ],
      "duration_seconds": 0.2449071669980185,
      "metrics": {
        "blocks_analyzed": 428,
        "cyclomatic_complexity_avg": 3.8,
        "cyclomatic_complexity_max": 19,
        "cyclomatic_complexity_rank": "C",
        "files_analyzed": 34,
        "maintainability_index_avg": 57.09,
        "maintainability_index_min": 0.0,
        "threshold_average": 3.6,
        "threshold_function": 12.0,
        "threshold_maintainability": 62.0
      },
      "status": "failed"
    },
    "db_setup": {
      "artifacts": {},
      "details": [
        "created:docdb_eval_20251018_162535_unknown"
      ],
      "duration_seconds": 0.06738437499734573,
      "metrics": {
        "database": "docdb_eval_20251018_162535_unknown"
      },
      "status": "passed"
    },
    "extraction": {
      "artifacts": {
        "fixtures": [
          {
            "document_id": "3061ea24-ee4b-4cff-be26-846865e76b4a",
            "fields": {
              "abstract": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data.",
              "authors": [
                "Ashish Vaswani",
                "Noam Shazeer",
                "Niki Parmar",
                "Jakob Uszkoreit",
                "Llion Jones",
                "Aidan N. Gomez",
                "Lukasz Kaiser",
                "Illia Polosukhin"
              ],
              "title": "Attention Is All You Need"
            }
          },
          {
            "document_id": "doc_batch_normalization",
            "fields": {
              "abstract": "Training deep neural networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin.",
              "authors": [
                "Sergey Ioffe",
                "Christian Szegedy"
              ],
              "title": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
            }
          },
          {
            "document_id": "doc_deep_residual_learning",
            "fields": {
              "abstract": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers, 8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers.",
              "authors": [
                "Kaiming He",
                "Xiangyu Zhang",
                "Shaoqing Ren",
                "Jian Sun"
              ],
              "title": "Deep Residual Learning for Image Recognition"
            }
          }
        ]
      },
      "details": [
        "extraction scoring not implemented"
      ],
      "duration_seconds": 0.0,
      "metrics": {
        "fixtures_total": 6
      },
      "status": "warn"
    },
    "ingestion": {
      "artifacts": {
        "documents": [
          "b52f04df-c688-4d96-9f78-27725ff56c3f",
          "671fbdaa-cca8-4d7a-88e2-2f93d21f1e12",
          "1b521f94-c21a-4d0f-a3c4-6d6bb7166334",
          "aaeffe9c-42a1-40cc-8e10-c2abcd354a7e",
          "10757e57-d90c-49c8-9beb-57eea45e5620",
          "d0397957-2ab0-461f-8d40-63c703473a94"
        ]
      },
      "details": [
        "ingested:b52f04df-c688-4d96-9f78-27725ff56c3f",
        "ingested:671fbdaa-cca8-4d7a-88e2-2f93d21f1e12",
        "ingested:1b521f94-c21a-4d0f-a3c4-6d6bb7166334",
        "ingested:aaeffe9c-42a1-40cc-8e10-c2abcd354a7e",
        "ingested:10757e57-d90c-49c8-9beb-57eea45e5620",
        "ingested:d0397957-2ab0-461f-8d40-63c703473a94"
      ],
      "duration_seconds": 0.0,
      "metrics": {
        "avg_time_per_doc_sec": 0.0,
        "document_ids": [
          "b52f04df-c688-4d96-9f78-27725ff56c3f",
          "671fbdaa-cca8-4d7a-88e2-2f93d21f1e12",
          "1b521f94-c21a-4d0f-a3c4-6d6bb7166334",
          "aaeffe9c-42a1-40cc-8e10-c2abcd354a7e",
          "10757e57-d90c-49c8-9beb-57eea45e5620",
          "d0397957-2ab0-461f-8d40-63c703473a94"
        ],
        "documents_attempted": 6,
        "documents_failed": 0,
        "documents_ingested": 6,
        "durations_sec": [
          0.0,
          0.0,
          0.0,
          0.0,
          0.0,
          0.0
        ],
        "ingest_time_per_page_sec": 0.0,
        "total_pages": 79,
        "total_time_sec": 0.0
      },
      "status": "passed"
    },
    "math": {
      "artifacts": {
        "fixtures": [
          {
            "document_id": "3061ea24-ee4b-4cff-be26-846865e76b4a",
            "id": "scaled-attention",
            "latex_gold": "\\mathrm{Attention}(Q,K,V)=\\mathrm{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V",
            "notes": "Scaled dot-product attention from Section 3.2."
          }
        ]
      },
      "details": [
        "math scoring not implemented"
      ],
      "duration_seconds": 0.0,
      "metrics": {
        "fixtures_total": 1
      },
      "status": "warn"
    },
    "perf": {
      "artifacts": {},
      "details": [],
      "duration_seconds": 0.0,
      "metrics": {
        "ingestion_avg_time_sec": 0.0,
        "ingestion_total_time_sec": 0.0,
        "query_p50_ms": 46.310312500281725,
        "query_p95_ms": 948.6303859957834
      },
      "status": "passed"
    },
    "queries": {
      "artifacts": {
        "queries": [
          {
            "hit_at_k": 0.0,
            "id": "attn-arch",
            "latency_ms": 2561.656709993258,
            "match_strategy": "passage",
            "ndcg_at_k": 0.8187761268552827,
            "query": "What architecture does the paper introduce?",
            "retrieved_fingerprints": [
              "cf7fcb7735b6c9f9db83de0a",
              "8d86e55903f99e37df417727",
              "ce49822ce8f3769d9c39f86c",
              "cf8f2daea9967287834f1bb2",
              "99f28cd92a139eada1660f11"
            ],
            "retrieved_snippets": [
              "Appendix Variant of the Inception Model Used Figure 5 documents the changes that were performed compared to the architecture with respect to the GoogleNet archictecture. For the interpretation of t...",
              "14 \u00d7 14 \u00d7 1024 3 0 128 192 192 256 max + pass through inception (5a) 7 \u00d7 7 \u00d7 1024 3 352 192 320 160 224 avg + 128 inception (5b) 7 \u00d7 7 \u00d7 1024 3 352 192 320 192 224 max + 128 avg pool 7 \u00d7 7 / 1 1 \u00d7...",
              "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Ne...",
              "on a 2-D plane were determined. Projections corresponding to the stable orientations for a 3-D aggregate were obtained as 2-D pixilated images and were analyzed for their projected area A agg , pro...",
              "\uf8f0 1 \u00d7 1, 256 3 \u00d7 3, 256 1 \u00d7 1, 1024 \uf8f9 \uf8fb \u00d7 23 \uf8ee \uf8f0 1 \u00d7 1, 256 3 \u00d7 3, 256 1 \u00d7 1, 1024 \uf8f9 \uf8fb \u00d7 36 conv5 x 7 \u00d7 7 \ufffd 3 \u00d7 3, 512 3 \u00d7 3, 512 \ufffd \u00d7 2 \ufffd 3 \u00d7 3, 512 3 \u00d7 3, 512 \ufffd \u00d7 3 \uf8ee \uf8f0 1 \u00d7 1, 512 3 \u00d7 3, 512 1 \u00d7 1..."
            ],
            "similarity_scores": [
              0.0783289817232376,
              0.16120906801007556,
              0.2230971128608924,
              0.04564315352697095,
              0.14300736067297581
            ],
            "top_similarity": 0.2230971128608924
          },
          {
            "hit_at_k": 0.0,
            "id": "encoder-depth",
            "latency_ms": 64.18741600646172,
            "match_strategy": "passage",
            "ndcg_at_k": 0.9917893032737543,
            "query": "How many layers are stacked in the Transformer encoder?",
            "retrieved_fingerprints": [
              "d6dc65fdefee7bb585aa29ac",
              "6c630f3becee38dac0f71091",
              "73b99b6d7996a29ff70d18e7",
              "674313ba419748feddb90a06",
              "9966ba8cbd02c5a7efb8a6ac"
            ],
            "retrieved_snippets": [
              "Figure 1: The Transformer - model architecture. The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decode...",
              "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position- wise...",
              "Most competitive neural sequence transduction models have an encoder-decoder structure [ 5 , 2 , 35 ]. Here, the encoder maps an input sequence of symbol representations ( x 1 , ..., x n ) to a seq...",
              "Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head...",
              "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output. The T..."
            ],
            "similarity_scores": [
              0.11871227364185111,
              0.08416547788873038,
              0.10035005834305717,
              0.08807896735003796,
              0.07447633824670287
            ],
            "top_similarity": 0.11871227364185111
          },
          {
            "hit_at_k": 0.0,
            "id": "multihead-uses",
            "latency_ms": 50.684207992162555,
            "match_strategy": "passage",
            "ndcg_at_k": 0.9954697773600235,
            "query": "List the three ways multi-head attention is used in the Transformer.",
            "retrieved_fingerprints": [
              "6238df9e872d44432f46741e",
              "674313ba419748feddb90a06",
              "c589795870462dbe067be80b",
              "7a47ff6624e0682b4e8a6262",
              "02b933cb09a7fcfcdd3eee26"
            ],
            "retrieved_snippets": [
              "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions. With a single attention head, averaging inhibits this. MultiHe...",
              "Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head...",
              "Figure 2: (left) Scaled Dot-Product Attention. (right) Multi-Head Attention consists of several attention layers running in parallel. of the values, where the weight assigned to each value is compu...",
              "Instead of performing a single attention function with d model -dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different,...",
              "The two most commonly used attention functions are additive attention [ 2 ], and dot-product (multi- plicative) attention. Dot-product attention is identical to our algorithm, except for the scalin..."
            ],
            "similarity_scores": [
              0.3760270662155631,
              0.013102480112306972,
              0.005208333333333333,
              0.0051387461459403904,
              0.01680672268907563
            ],
            "top_similarity": 0.3760270662155631
          },
          {
            "hit_at_k": 0.0,
            "id": "bn-internal-shift",
            "latency_ms": 38.344708998920396,
            "match_strategy": "passage",
            "ndcg_at_k": 0.9999901112763545,
            "query": "What phenomenon does batch normalization seek to reduce?",
            "retrieved_fingerprints": [
              "e32d9f1889d59cccc310a9e1",
              "1c152fffb8771b8f92eb08a7",
              "1136b15dad197a1d9c05619b",
              "4c3d92635d059c65665236a6",
              "0011f1f39398af60f3eb1e3d"
            ],
            "retrieved_snippets": [
              "In traditional deep networks, too-high learning rate may result in the gradients that explode or vanish, as well as getting stuck in poor local minima. Batch Normaliza- tion helps address these iss...",
              "Interestingly, our method bears similarity to the stan- dardization layer of (G\u00a8ulc\u00b8ehre & Bengio, 2013), though the two methods stem from very different goals, and per- form different tasks. The g...",
              "Simply adding Batch Normalization to a network does not take full advantage of our method. To do so, we further changed the network and its training parameters, as fol- lows: Increase learning rate...",
              "Simply adding Batch Normalization to a network does not take full advantage of our method. To do so, we further changed the network and its training parameters, as fol- lows: Increase learning rate...",
              "Simply adding Batch Normalization to a network does not take full advantage of our method. To do so, we further changed the network and its training parameters, as fol- lows: Increase learning rate..."
            ],
            "similarity_scores": [
              0.02463768115942029,
              0.015710919088766692,
              0.005373936408419167,
              0.00530035335689046,
              0.005309734513274336
            ],
            "top_similarity": 0.02463768115942029
          },
          {
            "hit_at_k": 0.0,
            "id": "resnet-imagenet-error",
            "latency_ms": 41.39254199981224,
            "match_strategy": "passage",
            "ndcg_at_k": 0.5637341844118282,
            "query": "What ImageNet test error does the residual net ensemble achieve?",
            "retrieved_fingerprints": [
              "298447dd7aae698defc16e1b",
              "aaa5215880b879b76b388ff1",
              "52614438d42c666ae0c541ab",
              "09b1f6d57f15e5bfb0d7b21f",
              "b4ed472b8491b8af2c4f2985"
            ],
            "retrieved_snippets": [
              "VGG [41] (ILSVRC\u201914) 7.32 GoogLeNet [44] (ILSVRC\u201914) 6.66 VGG [41] (v5) 6.8 PReLU-net [13] 4.94 BN-inception [16] 4.82 ResNet (ILSVRC\u201915) 3.57 Table 5. Error rates (%) of ensembles . The top-5 erro...",
              "Table 2. Top-1 error (%, 10-crop testing) on ImageNet validation. Here the ResNets have no extra parameter compared to their plain counterparts. Fig. 4 shows the training procedures. 34-layer plain...",
              "We evaluate our method on the ImageNet 2012 classi\ufb01- cation dataset [36] that consists of 1000 classes. The models are trained on the 1.28 million training images, and evalu- ated on the 50k valida...",
              "Table 1. Architectures for ImageNet. Building blocks are shown in brackets (see also Fig. 5), with the numbers of blocks stacked. Down- sampling is performed by conv3 1, conv4 1, and conv5 1 with a...",
              "Comparisons with State-of-the-art Methods. In Table 4 we compare with the previous best single-model results. Our baseline 34-layer ResNets have achieved very compet- itive accuracy. Our 152-layer..."
            ],
            "similarity_scores": [
              0.009395973154362415,
              0.010286554004408524,
              0.00844475721323012,
              0.015482054890921885,
              0.08054711246200608
            ],
            "top_similarity": 0.08054711246200608
          },
          {
            "hit_at_k": 0.0,
            "id": "bert-acronym",
            "latency_ms": 29.266709010698833,
            "match_strategy": "passage",
            "ndcg_at_k": 0.872942053263065,
            "query": "What does the acronym BERT stand for?",
            "retrieved_fingerprints": [
              "73f65e1171e68defaedf7fce",
              "6ec7abaf59cc31806b729ca4",
              "6a4787b852d330ddbb895246",
              "054d552919246f16dec9a96d",
              "6b707c8b7467006b9c2c4623"
            ],
            "retrieved_snippets": [
              "We organize the appendix into three sections: \u2022 Additional implementation details for BERT are presented in Appendix A ; \u2022 Additional details for our experiments are presented in Appendix B ; and \u2022...",
              "Ours BERT LARGE (Single) 78.7 81.9 80.0 83.1 Table 3: SQuAD 2.0 results. We exclude entries that use BERT as one of their components. tuning data, we only lose 0.1-0.4 F1, still outper- forming all...",
              "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Jacob Devlin Ming-Wei Chang Kenton Lee Kristina Toutanova Google AI Language { jacobdevlin,mingweichang,kentonl,kris...",
              "BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art re- sults on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point ab...",
              "\u2022 BERT advances the state of the art for eleven NLP tasks. The code and pre-trained mod- els are available at https://github.com/ google-research/bert . 2 Related Work There is a long history of pr..."
            ],
            "similarity_scores": [
              0.088715953307393,
              0.1291759465478842,
              0.17735334242837653,
              0.12460567823343849,
              0.1497872340425532
            ],
            "top_similarity": 0.17735334242837653
          },
          {
            "hit_at_k": 0.0,
            "id": "deeplab-decoder",
            "latency_ms": 41.936417008400895,
            "match_strategy": "passage",
            "ndcg_at_k": 0.8020027953057931,
            "query": "What module does DeepLabv3+ add to improve segmentation boundaries?",
            "retrieved_fingerprints": [
              "8c5072b45869404fb4df7d88",
              "064e41e7cd4619c0e1666bd6",
              "62b4abfa0e8afedf8969d0ee",
              "97ae199bf7c857897ded3712",
              "9c89a271a1df4a67971c174f"
            ],
            "retrieved_snippets": [
              "We de\ufb01ne \u201cDeepLabv3 feature map\u201d as the last feature map computed by DeepLabv3 ( i.e ., the features containing ASPP features and image-level fea- tures), and [ k \u00d7 k, f ] as a convolution operatio...",
              "We de\ufb01ne \u201cDeepLabv3 feature map\u201d as the last feature map computed by DeepLabv3 ( i.e ., the features containing ASPP features and image-level fea- tures), and [ k \u00d7 k, f ] as a convolution operatio...",
              "4 L.-C Chen, Y. Zhu, G. Papandreou, F. Schro\ufb00, and H. Adam 1x1 Conv 3x3 Conv rate 6 3x3 Conv rate 12 3x3 Conv rate 18 Image Pooling 1x1 Conv 1x1 Conv Low-Level Features Upsample by 4 Concat 3x3 Con...",
              "Abstract. Spatial pyramid pooling module or encode-decoder structure are used in deep neural networks for semantic segmentation task. The former networks are able to encode multi-scale contextual i...",
              "(a) val set results (b) test set results Table 7. (a) DeepLabv3+ on the Cityscapes val set when trained with train \ufb01ne set. (b) DeepLabv3+ on Cityscapes test set. Coarse : Use train extra set (coar..."
            ],
            "similarity_scores": [
              0.07967881408276714,
              0.08565737051792828,
              0.21940298507462686,
              0.1407437025189924,
              0.14545454545454545
            ],
            "top_similarity": 0.21940298507462686
          },
          {
            "hit_at_k": 0.0,
            "id": "cluster-overview",
            "latency_ms": 34.19500000018161,
            "match_strategy": "passage",
            "ndcg_at_k": 0.9664788812000285,
            "query": "what is this paper about",
            "retrieved_fingerprints": [
              "2d8c637d46f37af055d90add",
              "ce49822ce8f3769d9c39f86c",
              "fb9d716175a8547a673f0f6f",
              "cf8f2daea9967287834f1bb2",
              "efcc6db95b84d405db5b0867"
            ],
            "retrieved_snippets": [
              "Aerosol Science and Technology ISSN: 0278-6826 (Print) 1521-7388 (Online) Journal homepage: www.tandfonline.com/journals/uast20",
              "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Ne...",
              "on a 2-D plane were determined. Projections corresponding to the stable orientations for a 3-D aggregate were obtained as 2-D pixilated images and were analyzed for their projected area A agg , pro...",
              "on a 2-D plane were determined. Projections corresponding to the stable orientations for a 3-D aggregate were obtained as 2-D pixilated images and were analyzed for their projected area A agg , pro...",
              "L 2 d max = \u2212 1 . 11 \u2212 0 . 93 + 0 . 82 ( D f ) 2 + 5 . 1751 + 3 . 28 + 3 . 00 ( D f ) \u2212 5 . 028 \u2212 2 . 78 + 2 . 61 , 1 . 5 < D f \u2264 2 . 0; R 2 = 0 . 99 , [8] W 2 d max L 2 d max = 0 . 75 + 0 . 11 \u2212 0..."
            ],
            "similarity_scores": [
              0.17197452229299362,
              0.1559748427672956,
              0.10553278688524591,
              0.18455366098294884,
              0.15547024952015356
            ],
            "top_similarity": 0.18455366098294884
          },
          {
            "hit_at_k": 0.0,
            "id": "cluster-abstract",
            "latency_ms": 29.11162500095088,
            "match_strategy": "passage",
            "ndcg_at_k": 0.8809501378997627,
            "query": "what is the abstract of this paper",
            "retrieved_fingerprints": [
              "ce49822ce8f3769d9c39f86c",
              "fb9d716175a8547a673f0f6f",
              "cf8f2daea9967287834f1bb2",
              "7c6d5ef7d39da851f4ab3e54",
              "09c8b3fd7aa0a6d657e50e57"
            ],
            "retrieved_snippets": [
              "Provided proper attribution is provided, Google hereby grants permission to reproduce the tables and figures in this paper solely for use in journalistic or scholarly works. Attention Is All You Ne...",
              "on a 2-D plane were determined. Projections corresponding to the stable orientations for a 3-D aggregate were obtained as 2-D pixilated images and were analyzed for their projected area A agg , pro...",
              "on a 2-D plane were determined. Projections corresponding to the stable orientations for a 3-D aggregate were obtained as 2-D pixilated images and were analyzed for their projected area A agg , pro...",
              "where the superscripts and the subscripts correspond to the difference between upper and lower quartile values and the mean, respectively. It is interesting to note that the as- pect ratio approach...",
              "\u2248 1.2\u20131.5) being produced from a premixed \ufb02ame (Chakrabarty et al. 2009b). Given the varied morphology that aggregates can assume, experimental techniques to characterize their morphology is of pri..."
            ],
            "similarity_scores": [
              0.009708737864077669,
              0.009086320040383645,
              0.017543859649122806,
              0.008454106280193236,
              0.004492362982929021
            ],
            "top_similarity": 0.017543859649122806
          },
          {
            "hit_at_k": 0.0,
            "id": "cluster-regime-definition",
            "latency_ms": 27.64487499371171,
            "match_strategy": "passage",
            "ndcg_at_k": 0.5760337376840909,
            "query": "What is the cluster dliute regime?",
            "retrieved_fingerprints": [
              "6b4f9210342b1a24062e8dc3",
              "087cb1c07156435df7f9d2a2",
              "28d4d01a003b75e4ebcb5b69",
              "86d70eadf87547067405f4df",
              "8e3513049139766198a0b779"
            ],
            "retrieved_snippets": [
              "2. SIMULATION AND ANALYSIS OF AGGREGATES The simulation technique used for this study makes use of a recently developed simulation package, FracMAP (Chakrabarty et al. 2008, 2009a), which generates...",
              "2. SIMULATION AND ANALYSIS OF AGGREGATES The simulation technique used for this study makes use of a recently developed simulation package, FracMAP (Chakrabarty et al. 2008, 2009a), which generates...",
              "Simulation of Aggregates with Point-Contacting Monomers in the Cluster\u2013Dilute Regime. Part 2: Comparison of Two- and Three-Dimensional Structural Properties as a Function of Fractal Dimension Rajan...",
              "where k 0 is the fractal prefactor, d p is the average monomer diameter, N is the number of monomers, and R 3 d g is the ra- dius of gyration of the aggregate. Laboratory investigations and compute...",
              "where k 0 is the fractal prefactor, d p is the average monomer diameter, N is the number of monomers, and R 3 d g is the ra- dius of gyration of the aggregate. Laboratory investigations and compute..."
            ],
            "similarity_scores": [
              0.0005564830272676684,
              0.00045672527974423386,
              0.025049439683586024,
              0.0059931506849315065,
              0.018995929443690638
            ],
            "top_similarity": 0.025049439683586024
          },
          {
            "hit_at_k": 0.0,
            "id": "cluster-morphology-formula",
            "latency_ms": 55.428082996513695,
            "match_strategy": "passage",
            "ndcg_at_k": 0.8874087203838014,
            "query": "What formula represents the morphological parameters of a 3-d aggregate?",
            "retrieved_fingerprints": [
              "bb7615519acfaad53c3c71f0",
              "7c6d5ef7d39da851f4ab3e54",
              "09c8b3fd7aa0a6d657e50e57",
              "c896fec130204d9e931f174f",
              "f9715474383b7aa92c5a640a"
            ],
            "retrieved_snippets": [
              "4. CONCLUSIONS In this present work, the nongeneralizability of often-used empirical relationships relating 3-D and projected properties of cluster\u2013dilute aggregates has been highlighted. This was...",
              "where the superscripts and the subscripts correspond to the difference between upper and lower quartile values and the mean, respectively. It is interesting to note that the as- pect ratio approach...",
              "\u2248 1.2\u20131.5) being produced from a premixed \ufb02ame (Chakrabarty et al. 2009b). Given the varied morphology that aggregates can assume, experimental techniques to characterize their morphology is of pri...",
              "1. INTRODUCTION The three-dimensional (3-D) fractal dimension, D f , is consid- ered to be the key property for de\ufb01ning the complex morphology of fractal-like aggregates made of coagulated spherica...",
              "(1) For any given D f , the R 3 d g value of a 3-D aggregate is always greater than its projected R 2 d g values, and many overlapping values of R 2 d g \ufffd R 3 d g exist for varying values of D f ...."
            ],
            "similarity_scores": [
              0.10665408211420481,
              0.10899873257287707,
              0.025454545454545455,
              0.1564625850340136,
              0.0853960396039604
            ],
            "top_similarity": 0.1564625850340136
          },
          {
            "hit_at_k": 0.0,
            "id": "cluster-radius-ratio",
            "latency_ms": 80.07774999714456,
            "match_strategy": "passage",
            "ndcg_at_k": 0.9909897325626612,
            "query": "What did they calculate the ratio of the 2D radius of gyration divided by the 3D radius of gyration to be in the experimental results?",
            "retrieved_fingerprints": [
              "d55be27a439108aca8bc26b7",
              "e388e31ace85fb31ca5fb5d1",
              "24917d7d0b7111f8f3dec42c",
              "0476b8b903951b46949fed93",
              "819e13b2935aabdb6f838c89"
            ],
            "retrieved_snippets": [
              "results are still far worse than those of the pre- trained bidirectional models. The BiLSTM hurts performance on the GLUE tasks. We recognize that it would also be possible to train separate LTR an...",
              "The analogous formula is used for the end of the answer span. The score of a candidate span from position i to position j is de\ufb01ned as S \u00b7 T i + E \u00b7 T j , and the maximum scoring span where j \u2265 i i...",
              "We use a batch size of 32 and \ufb01ne-tune for 3 epochs over the data for all GLUE tasks. For each task, we selected the best \ufb01ne-tuning learning rate (among 5e-5, 4e-5, 3e-5, and 2e-5) on the Dev set....",
              "The analogous formula is used for the end of the answer span. The score of a candidate span from position i to position j is de\ufb01ned as S \u00b7 T i + E \u00b7 T j , and the maximum scoring span where j \u2265 i i...",
              "The analogous formula is used for the end of the answer span. The score of a candidate span from position i to position j is de\ufb01ned as S \u00b7 T i + E \u00b7 T j , and the maximum scoring span where j \u2265 i i..."
            ],
            "similarity_scores": [
              0.17212121212121212,
              0.0988857938718663,
              0.07119314436387607,
              0.10085227272727272,
              0.09073482428115016
            ],
            "top_similarity": 0.17212121212121212
          },
          {
            "hit_at_k": 0.0,
            "id": "compare-transformer-bert",
            "latency_ms": 61.96733299293555,
            "match_strategy": "passage",
            "ndcg_at_k": 0.9939022215681448,
            "query": "How does \"Attention Is All You Need\" compare with the BERT paper in encoder depth and pre-training objective?",
            "retrieved_fingerprints": [
              "a7682e505c9e8baec6a2162f",
              "95fe392de71b46fa094be375",
              "f35261d35b9248ef3fec585c",
              "054d552919246f16dec9a96d",
              "45d9a2bc8f7961f811987805"
            ],
            "retrieved_snippets": [
              "Dev Set Tasks MNLI-m QNLI MRPC SST-2 SQuAD (Acc) (Acc) (Acc) (Acc) (F1) BERT BASE 84.4 88.4 86.7 92.7 88.5 No NSP 83.9 84.9 86.5 92.6 87.9 LTR & No NSP 82.1 84.3 77.5 92.1 77.8 + BiLSTM 82.1 84.1 7...",
              "ablation studies can be found in Appendix C . 5.1 Effect of Pre-training Tasks We demonstrate the importance of the deep bidi- rectionality of BERT by evaluating two pre- training objectives using...",
              "Fine-tuning is straightforward since the self- attention mechanism in the Transformer al- lows BERT to model many downstream tasks\u2014 whether they involve single text or text pairs\u2014by swapping out th...",
              "BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art re- sults on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point ab...",
              "3 768 12 5.84 77.9 79.8 88.4 6 768 3 5.24 80.6 82.2 90.7 6 768 12 4.68 81.9 84.8 91.3 12 768 12 3.99 84.4 86.7 92.9 12 1024 16 3.54 85.7 86.9 93.3 24 1024 16 3.23 86.6 87.8 93.7 Table 6: Ablation o..."
            ],
            "similarity_scores": [
              0.08599779492833518,
              0.05565638233514821,
              0.04377880184331797,
              0.05704697986577181,
              0.031088082901554404
            ],
            "top_similarity": 0.08599779492833518
          },
          {
            "hit_at_k": 0.0,
            "id": "compare-resnet-batchnorm",
            "latency_ms": 52.79800000425894,
            "match_strategy": "passage",
            "ndcg_at_k": 0.918959654623109,
            "query": "Contrast how the ResNet paper and the Batch Normalization paper improve deep network training.",
            "retrieved_fingerprints": [
              "af640f93f33c58fa3044080d",
              "f221ba4a0c31be03619acb63",
              "34e18168a6b8f36120411aed",
              "7cff9195631d04db9c2189fd",
              "5788cf1f1d180a600c7083fd"
            ],
            "retrieved_snippets": [
              "plies to sub-networks and layers, and removing it from internal activations of the network may aid in training. Our proposed method draws its power from normalizing activations, and from incorporat...",
              "We refer to the change in the distributions of internal nodes of a deep network, in the course of training, as In- ternal Covariate Shift . Eliminating it offers a promise of faster training. We pr...",
              "Abstract Training Deep Neural Networks is complicated by the fact that the distribution of each layer\u2019s inputs changes during training, as the parameters of the previous layers change. This slows d...",
              "Figure 4: Batch-Normalized Inception comparison with previous state of the art on the provided validation set com- prising 50000 images. *BN-Inception ensemble has reached 4.82% top-5 error on the...",
              "3 fully-connected hidden layers with 100 activations each. Each hidden layer computes y = g ( W u+b) with sigmoid nonlinearity, and the weights W initialized to small ran- dom Gaussian values. The..."
            ],
            "similarity_scores": [
              0.10691458454386985,
              0.17215189873417722,
              0.12090313182811362,
              0.1518987341772152,
              0.08729812309035356
            ],
            "top_similarity": 0.17215189873417722
          }
        ]
      },
      "details": [
        "hit@5 below threshold 0.000< 0.7",
        "avg top similarity 0.145< 0.82"
      ],
      "duration_seconds": 3.1686913769954117,
      "metrics": {
        "hit_at_k_avg": 0.0,
        "latency_ms_all": [
          2561.656709993258,
          64.18741600646172,
          50.684207992162555,
          38.344708998920396,
          41.39254199981224,
          29.266709010698833,
          41.936417008400895,
          34.19500000018161,
          29.11162500095088,
          27.64487499371171,
          55.428082996513695,
          80.07774999714456,
          61.96733299293555,
          52.79800000425894
        ],
        "latency_p50_ms": 46.310312500281725,
        "latency_p95_ms": 948.6303859957834,
        "ndcg_at_k_avg": 0.8756733884048359,
        "queries_run": 14,
        "top_similarity_all": [
          0.2230971128608924,
          0.11871227364185111,
          0.3760270662155631,
          0.02463768115942029,
          0.08054711246200608,
          0.17735334242837653,
          0.21940298507462686,
          0.18455366098294884,
          0.017543859649122806,
          0.025049439683586024,
          0.1564625850340136,
          0.17212121212121212,
          0.08599779492833518,
          0.17215189873417722
        ],
        "top_similarity_avg": 0.14526128749829517
      },
      "status": "failed"
    }
  },
  "timestamp": "2025-10-18T16:29:37.794918+00:00"
}